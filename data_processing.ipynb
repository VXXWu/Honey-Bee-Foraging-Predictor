{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"/Users/vincewu/Bees-Project\"\n",
    "processed_folder = 'Fully Processed'\n",
    "processed_file = 'forager_data.csv'\n",
    "\n",
    "processed_csv = pd.read_csv(os.path.join(directory, processed_folder, processed_file), engine='python', header=None)\n",
    "\n",
    "headers=processed_csv.loc[0].tolist()\n",
    "headers[0]='Numbers'\n",
    "\n",
    "processed_csv.columns=headers\n",
    "processed_csv=processed_csv.drop(labels=0, axis=0)\n",
    "processed_csv.index-=1\n",
    "processed_csv['Date']=pd.to_datetime(processed_csv['Date'])\n",
    "# processed_csv['Date']=pd.to_datetime(processed_csv['Date'])\n",
    "processed_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dates=processed_csv['Date'].unique()\n",
    "all_dates=set()\n",
    "for i in range(len(processed_csv)):\n",
    "    all_dates.add(processed_csv['Date'][i])\n",
    "\n",
    "all_dates=sorted(all_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = {}\n",
    "factors=5\n",
    "for i in all_dates:\n",
    "    arr[i] = {'Active Bees': 0, 'Trips Taken': 0, 'Total Bees': 0, 'Minutes Outside':\n",
    "              [0 for x in range(24)], 'Bees Outside': [0 for x in range(24)], 'Egress':\n",
    "              [0 for x in range(24)], 'Ingress': [0 for x in range(24)], \n",
    "            'Egress and Ingress': [0 for x in range(24)]}\n",
    "#percentage of hour outside per bee, bees outside per bee, ingresses or egresses per be\n",
    "uids = set()\n",
    "sighted = {}\n",
    "for i in range(len(processed_csv)):\n",
    "    uid = processed_csv['UID'][i]\n",
    "    if not uid in uids:\n",
    "        uids.add(uid)\n",
    "        sighted[uid] = [processed_csv['Date'][i]]\n",
    "    else:\n",
    "        sighted[uid].append(processed_csv['Date'][i])\n",
    "\n",
    "for uid in sighted:\n",
    "    arr[sighted[uid][0]]['Total Bees'] += 1\n",
    "    check = False\n",
    "    for date in all_dates:\n",
    "        if check:\n",
    "            arr[date]['Total Bees'] -= 1\n",
    "            break\n",
    "        if date == sighted[uid][-1]:\n",
    "            check = True\n",
    "\n",
    "running=0\n",
    "for date in arr:\n",
    "    running+=arr[date]['Total Bees']\n",
    "    arr[date]['Total Bees']=running\n",
    "\n",
    "for i in range(len(processed_csv)):\n",
    "    trip_amount = int(processed_csv['Trip Amount'][i])\n",
    "\n",
    "    date = processed_csv['Date'][i]\n",
    "    arr[date]['Active Bees'] += 1\n",
    "    arr[date]['Trips Taken'] += trip_amount\n",
    "\n",
    "    trips = processed_csv['Trip Start and End Times (Hours)'][i]\n",
    "    trips = trips.replace('[', '').replace(']', '').split(', ')\n",
    "\n",
    "    rounded_trips = trips.copy()\n",
    "\n",
    "    for j in range(len(rounded_trips)):\n",
    "        for k in range(len(rounded_trips[j])):\n",
    "            if rounded_trips[j][k] == '.':\n",
    "                index = k\n",
    "                break\n",
    "        rounded_trips[j] = int(rounded_trips[j][:index])\n",
    "\n",
    "    for j in range(len(trips)):\n",
    "        trips[j] = float(trips[j])\n",
    "\n",
    "    for j in range(len(trips)):\n",
    "        if j % 2 == 1:\n",
    "            trip_start_hour = rounded_trips[j-1]\n",
    "            trip_end_hour = rounded_trips[j]\n",
    "\n",
    "            arr[date]['Egress'][trip_start_hour] += 1\n",
    "            arr[date]['Ingress'][trip_end_hour] += 1\n",
    "            arr[date]['Egress and Ingress'][trip_start_hour] += 1\n",
    "            arr[date]['Egress and Ingress'][trip_end_hour] += 1\n",
    "\n",
    "            arr[date]['Minutes Outside'][trip_start_hour] += trips[j-1] - \\\n",
    "                trip_start_hour\n",
    "            arr[date]['Minutes Outside'][trip_end_hour] += trips[j]-trip_end_hour\n",
    "            for hour in range(trip_start_hour+1, trip_end_hour):\n",
    "                arr[date]['Minutes Outside'][hour] += 1\n",
    "\n",
    "    outside = [False for hour in range(24)]\n",
    "    for j in range(len(rounded_trips)):\n",
    "        if j % 2 == 1:\n",
    "            for k in range(rounded_trips[j-1], rounded_trips[j]+1):\n",
    "                outside[k] = True\n",
    "\n",
    "    for hour in range(len(outside)):\n",
    "        if outside[hour]:\n",
    "            arr[date]['Bees Outside'][hour] += 1\n",
    "    # if outside[0] or outside[1] or outside[2]:\n",
    "    #     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in arr:\n",
    "    for hour in range(24): \n",
    "        arr[key]['Bees Outside'][hour]/=arr[key]['Total Bees']\n",
    "        arr[key]['Minutes Outside'][hour]/=arr[key]['Total Bees']\n",
    "        arr[key]['Egress'][hour]/=arr[key]['Trips Taken']\n",
    "        arr[key]['Ingress'][hour]/=arr[key]['Trips Taken']\n",
    "        arr[key]['Egress and Ingress'][hour]/=arr[key]['Trips Taken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blocks=[]\n",
    "prev_date=list(arr.keys())[0]\n",
    "data_blocks.append([prev_date])\n",
    "for i in range(1, len(arr)):\n",
    "    date=list(arr.keys())[i]\n",
    "    if date-prev_date>pd.Timedelta(days=10):\n",
    "        data_blocks.append([date])\n",
    "    else:\n",
    "        data_blocks[-1].append(date)\n",
    "    prev_date = date\n",
    "\n",
    "curr_len=len(data_blocks)\n",
    "data_blocks.append([])\n",
    "block_num = {}\n",
    "for i in range(curr_len):\n",
    "    for date in data_blocks[i]:\n",
    "        block_num[date] = i\n",
    "        data_blocks[-1].append(date)\n",
    "\n",
    "blocks = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orientation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#percentage of hour outside per bee, bees outside per bee, ingresses or egresses per be\n",
    "# uids = set()\n",
    "# sighted = {}\n",
    "# for i in range(len(processed_csv)):\n",
    "#     uid = processed_csv['UID'][i]\n",
    "#     if not uid in uids:\n",
    "#         uids.add(uid)\n",
    "#         sighted[uid] = [processed_csv['Date'][i]]\n",
    "#     else:\n",
    "#         sighted[uid].append(processed_csv['Date'][i])\n",
    "\n",
    "# for uid in sighted:\n",
    "#     arr[sighted[uid][0]]['Total Bees'] += 1\n",
    "#     check = False\n",
    "#     for date in all_dates:\n",
    "#         if check:\n",
    "#             arr[date]['Total Bees'] -= 1\n",
    "#             break\n",
    "#         if date == sighted[uid][-1]:\n",
    "#             check = True\n",
    "\n",
    "# running = 0\n",
    "# for date in arr:\n",
    "#     running += arr[date]['Total Bees']\n",
    "#     arr[date]['Total Bees'] = running\n",
    "\n",
    "first_trip = [[0 for hour in range(24)] for i in range(3)]\n",
    "last_trip = [[0 for hour in range(24)] for i in range(3)]\n",
    "# first_trip = [pd.DataFrame(columns=data_blocks[i]) for i in range(3)]\n",
    "# last_trip = [pd.DataFrame(columns=data_blocks[i]) for i in range(3)]\n",
    "total_trips=[0, 0, 0]\n",
    "\n",
    "for i in range(len(processed_csv)):\n",
    "    # trip_amount = int(processed_csv['Trip Amount'][i])\n",
    "\n",
    "    date = processed_csv['Date'][i]\n",
    "\n",
    "    trips = processed_csv['Trip Start and End Times (Hours)'][i]\n",
    "    trips = trips.replace('[', '').replace(']', '').split(', ')\n",
    "\n",
    "    rounded_trips = trips.copy()\n",
    "\n",
    "    for j in range(len(rounded_trips)):\n",
    "        for k in range(len(rounded_trips[j])):\n",
    "            if rounded_trips[j][k] == '.':\n",
    "                index = k\n",
    "                break\n",
    "        rounded_trips[j] = int(rounded_trips[j][:index])\n",
    "\n",
    "    if not date in block_num:\n",
    "        continue\n",
    "    season=block_num[date]\n",
    "    first_trip[season][rounded_trips[0]] += 1\n",
    "    last_trip[season][rounded_trips[-1]] += 1\n",
    "    total_trips[season]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    for hour in range(24):\n",
    "        first_trip[season][hour] /= total_trips[season]\n",
    "        last_trip[season][hour] /= total_trips[season]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(first_trip[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pd.DataFrame(first_trip[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(first_trip_saved[0])\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Distribution of Forager Initial Departure Times (Summer)')\n",
    "ax.set_xticks([i for i in range(0, 24, 2)])\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'forager_depart_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(last_trip_saved[0])\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Distribution of Forager Final Return Times (Summer)')\n",
    "ax.set_xticks([i for i in range(0, 24, 2)])\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'forager_return_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "sns.lineplot(data=pd.DataFrame(last_trip_saved[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(saved_last_trip[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_trip_saved=first_trip\n",
    "last_trip_saved=last_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pd.DataFrame(first_trip[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pd.DataFrame(last_trip[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orientation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation_leave=pd.read_csv(os.path.join(directory, processed_folder, 'Time2 (OL).csv'))\n",
    "orientation_return=pd.read_csv(os.path.join(directory, processed_folder, 'Time3 (OR).csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_hours=[0 for i in range(24)]\n",
    "return_hours=[0 for i in range(24)]\n",
    "for i in range(len(orientation_leave)):\n",
    "    leave_hours[int(orientation_leave.iloc[i, 0])]+=1\n",
    "for i in range(len(orientation_return)):\n",
    "    return_hours[int(orientation_return.iloc[i, 0])]+=1\n",
    "\n",
    "for i in range(24):\n",
    "    leave_hours[i]/=len(orientation_leave)\n",
    "    return_hours[i]/=len(orientation_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "pd.DataFrame(last_trip_saved[0], columns=['Foraging']).plot(ax=ax, legend=True)\n",
    "pd.DataFrame(return_hours, columns=['Orientation']).plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Distribution of Final Bee Arrival Times (Summer)')\n",
    "ax.set_xticks([i for i in range(0, 24, 2)])\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'return_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "pd.DataFrame(first_trip_saved[0], columns=['Foraging']).plot(ax=ax, legend=True)\n",
    "pd.DataFrame(leave_hours, columns=['Orientation']).plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Distribution of Initial Bee Deparature Times (Summer)')\n",
    "ax.set_xticks([i for i in range(0, 24, 2)])\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'depart_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    for hour in range(24):\n",
    "        first_trip[season][hour] /= total_trips[season]\n",
    "        last_trip[season][hour] /= total_trips[season]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palettes=[]\n",
    "for season in range(3):\n",
    "    sns.scatterplot(data=pd.DataFrame(first_trip[season]))\n",
    "    sns.scatterplot(data=pd.DataFrame(last_trip[season]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weather file read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "months_num = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "weather_headers = ['Temp.', 'Dew Point', 'Humidity', 'Speed',\n",
    "           'Gust', 'Pressure', 'Rainfall', 'UV', 'Solar']\n",
    "\n",
    "weather_file = 'weather.csv'\n",
    "weather_data=pd.read_csv(os.path.join(directory, processed_folder, weather_file), \n",
    "        parse_dates=['Datetime'], engine='python')\n",
    "weather_data = weather_data.resample('1h', on='Datetime').mean()\n",
    "weather_data.columns=weather_headers\n",
    "weather_data.insert(len(weather_data.columns),\n",
    "                    'Datetime_col', weather_data.index.tolist())\n",
    "weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_indices=weather_data.index\n",
    "index=0\n",
    "\n",
    "weather_np = [pd.DataFrame() for i in range(blocks)]\n",
    "bee_np = [np.array([]) for i in range(blocks)]\n",
    "\n",
    "for key in sorted(arr):\n",
    "    while index<len(weather_indices) and weather_indices[index]!=key:\n",
    "        index+=1\n",
    "    temp_hour=0\n",
    "    while index<len(weather_indices) and weather_indices[index].day==key.day:\n",
    "        # weather_np.append([weather_data.iloc[index][i] for i in range(10)])\n",
    "        # weather_np = pd.concat([weather_np, pd.DataFrame(weather_data.iloc[index]).transpose()], ignore_index = True)\n",
    "        if not pd.isna(weather_data.iloc[index, 1]):\n",
    "            num=block_num[pd.to_datetime(str(key)[:10])]\n",
    "            weather_np[num] = pd.concat([weather_np[num], pd.DataFrame(weather_data.iloc[index]).transpose()], ignore_index=True)\n",
    "            bee_np[num] = np.append(bee_np[num], [arr[key]['Minutes Outside'][temp_hour], \n",
    "            arr[key]['Bees Outside'][temp_hour],\n",
    "            arr[key]['Egress'][temp_hour], arr[key]['Ingress'][temp_hour], \n",
    "            arr[key]['Egress and Ingress'][temp_hour],\n",
    "            key])\n",
    "\n",
    "            weather_np[-1] = pd.concat([weather_np[-1], pd.DataFrame(\n",
    "                weather_data.iloc[index]).transpose()], ignore_index=True)\n",
    "            bee_np[-1] = np.append(bee_np[-1], [arr[key]['Minutes Outside'][temp_hour], \n",
    "            arr[key]['Bees Outside'][temp_hour],\n",
    "            arr[key]['Egress'][temp_hour], arr[key]['Ingress'][temp_hour], \n",
    "            arr[key]['Egress and Ingress'][temp_hour],\n",
    "            key])\n",
    "        else:\n",
    "            print(index, weather_data.iloc[index])\n",
    "        # weather_np[index]=weather_data.iloc[index]\n",
    "        # weather_np=np.append(weather_np, [weather_data.iloc[index].to_numpy()])\n",
    "        index+=1\n",
    "        temp_hour+=1\n",
    "    \n",
    "for i in range(blocks):\n",
    "    weather_np[i] = weather_np[i].to_numpy()\n",
    "    bee_np[i] = np.reshape(bee_np[i], (int(len(bee_np[i])/(factors+1)), factors+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weather_data)):\n",
    "    if pd.isna(weather_data.iloc[i, 1]):\n",
    "        print(i, weather_data.iloc[i].name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_columns=['Temperature', 'Humidity', 'Speed', 'Pressure', 'Precip. Rate.', 'Solar']\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "temp = ['Temperature', 'Dew Point', 'Humidity', 'Speed',\n",
    "        'Gust', 'Pressure', 'Rainfall', 'UV', 'Solar']\n",
    "bee_factors = ['Minutes Outside', 'Bees Outside',\n",
    "               'Egress', 'Ingress', 'Egress and Ingress']\n",
    "\n",
    "fig_list = [pd.DataFrame(columns=bee_factors,\n",
    "                         index=temp) for i in range(len(data_blocks))]\n",
    "\n",
    "#categorical features with weather\n",
    "\n",
    "for block in range(len(data_blocks)):\n",
    "    print('block', block)\n",
    "    for i in range(factors):\n",
    "        for j in range(len(weather_data.columns)-1):\n",
    "            print(pearsonr(bee_np[block][:, i], weather_np[block][:, j]))\n",
    "            corr = pearsonr(bee_np[block][:, i],\n",
    "                       weather_np[block][:, j])[0]\n",
    "            if abs(corr) >= 0.5:\n",
    "                print(\"correlated\")\n",
    "\n",
    "            fig_list[block][bee_factors[i]][temp[j]] = corr\n",
    "\n",
    "            print(bee_factors[i], ';', temp[j])\n",
    "            # plt.scatter(final_hourly_bee_np[:,i], weather_data[:,j])\n",
    "            # plt.show\n",
    "\n",
    "for i in range(len(data_blocks)):\n",
    "    fig_list[i]=fig_list[i].astype(float)\n",
    "\n",
    "for i in range(len(fig_list[0].loc['Rainfall'])):\n",
    "    fig_list[0].loc['Rainfall'][i]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 8)}, style='white', font='Arial', font_scale=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[3], cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation of All Weather and Foraging Activity Factors (All Seasons)')\n",
    "plt.xticks(rotation=45)\n",
    "# plt.savefig(os.path.join(directory, 'Figures',\n",
    "#             'correlation_all.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[3], cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation of All Weather and Foraging Activity Factors (All Seasons)')\n",
    "plt.xticks(rotation=45)\n",
    "# plt.savefig(os.path.join(directory, 'Figures',\n",
    "#             'correlation_all.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_columns=['Temperature', 'Humidity', 'Speed', 'Pressure', 'Precip. Rate.', 'Solar']\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "temp = ['Temperature', 'Dew Point', 'Humidity', 'Speed',\n",
    "        'Gust', 'Pressure', 'Rainfall', 'UV', 'Solar']\n",
    "# bee_factor 'Bees Outside'\n",
    "# bee_factor=1\n",
    "\n",
    "fig_list = [pd.DataFrame(columns=bee_factors,\n",
    "                         index=temp) for i in range(len(data_blocks))]\n",
    "\n",
    "bees_outside_corr=pd.DataFrame(columns=seasons, index=temp)\n",
    "\n",
    "#categorical features with weather\n",
    "\n",
    "for block in range(len(data_blocks)-1):\n",
    "    print('block', block)\n",
    "    for j in range(len(weather_data.columns)-1):\n",
    "        print(pearsonr(bee_np[block][:, 1], weather_np[block][:, j]))\n",
    "        corr = pearsonr(bee_np[block][:, 1],\n",
    "                    weather_np[block][:, j])[0]\n",
    "        if abs(corr) >= 0.5:\n",
    "            print(\"correlated\")\n",
    "\n",
    "        bees_outside_corr.iloc[j, block] = corr\n",
    "\n",
    "        print(temp[j])\n",
    "            # plt.scatter(final_hourly_bee_np[:,i], weather_data[:,j])\n",
    "            # plt.show\n",
    "\n",
    "bees_outside_corr=bees_outside_corr.astype(float)\n",
    "\n",
    "bees_outside_corr.loc['Rainfall'][0]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (8, 6)}, style='white', font='Arial', font_scale=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=bees_outside_corr, cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation of Weather and Bees Outside Across Seasons')\n",
    "plt.xticks(rotation=45)\n",
    "# plt.savefig(os.path.join(directory, 'Figures',\n",
    "#             'correlation_bees_outside.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[0], cmap='coolwarm', annot=True)\n",
    "plt.title('Correlation of Weather and Foraging Activity Factors (Summer)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'correlation_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[1], annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation of Weather and Foraging Activity Factors (Fall)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'correlation_fall.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[2], annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\n",
    "    'Correlation of Weather and Foraging Activity Factors (Spring Without Rainy Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'correlation_spring_no_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=fig_list[2], annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\n",
    "    'Correlation of Weather and Foraging Activity Factors (Spring With Rainy Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'correlation_spring_with_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be data_blocks\n",
    "# num=2\n",
    "average_outside=[[0 for i in range(24)] for i in range(len(data_blocks)-1)]\n",
    "for i in range(len(data_blocks)-1):\n",
    "    for date in data_blocks[i]:\n",
    "        for hour in range(24):\n",
    "            # print(i, hour)\n",
    "            average_outside[i][hour]+=arr[date]['Bees Outside'][hour]\n",
    "\n",
    "for i in range(len(data_blocks)-1):\n",
    "    for hour in range(24):\n",
    "        average_outside[i][hour]/=len(data_blocks[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df=pd.DataFrame(average_outside).transpose()\n",
    "# plt_df.index=[int(i) for i in range(24)]\n",
    "plt_df.columns=['Summer', 'Fall', 'Early Spring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 5)}, style='white', font='Arial', font_scale=1.7)\n",
    "# sns.set_style()\n",
    "sns.scatterplot(data=plt_df)\n",
    "plt.title('Average Hourly Percentage of Bees Outside')\n",
    "plt.xticks([int(i) for i in range(0, 24, 2)])\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Percentage of Bees Outside')\n",
    "plt.savefig(os.path.join(directory, 'Figures', 'average_outside.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df=pd.DataFrame(weather_np, columns=weather_data.columns[:-1])\n",
    "bee_df=pd.DataFrame(bee_np, columns=['Minutes Outside', 'Bees Outside'])\n",
    "graph_df = pd.concat([weather_df, bee_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.DataFrame(weather_np[3])\n",
    "temp=temp.drop(columns=len(weather_np[3][0])-1)\n",
    "temp.columns=weather_data.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def calculate_vif(df, features):   \n",
    "    for feature in features:\n",
    "        X = [f for f in features if f != feature]        \n",
    "        X, y = df[X], df[feature]\n",
    "        r2 = LinearRegression().fit(X, y).score(X, y)\n",
    "        vif=1/(1 - r2)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_vifs=pd.DataFrame(columns=['Season', 'Input', 'Output', 'VIF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs=[]\n",
    "\n",
    "season=3\n",
    "\n",
    "temp = pd.DataFrame(weather_np[season][:, [i for i in range(len(weather_np[3][0])-1)]],\n",
    "                    columns=weather_headers)\n",
    "\n",
    "vif_df = pd.DataFrame(columns=temp.columns, index=temp.columns)\n",
    "for col1 in weather_headers:\n",
    "    for col2 in weather_headers:\n",
    "        # print(col1, col2)\n",
    "        if col1==col2:\n",
    "            vif_df.loc[col1, col2]='und.'\n",
    "        else:\n",
    "            vif_df.loc[col1, col2]=calculate_vif(temp, [col1, col2])\n",
    "\n",
    "vifs.append(vif_df)\n",
    "\n",
    "# for season in range(3):\n",
    "#     temp = pd.DataFrame(weather_np[season][:, [i for i in range(len(weather_np[3][0])-1)]],\n",
    "#                         columns=weather_headers)\n",
    "\n",
    "#     vif_df = pd.DataFrame(columns=temp.columns, index=temp.columns)\n",
    "#     for col1 in weather_headers:\n",
    "#         for col2 in weather_headers:\n",
    "#             # print(col1, col2)\n",
    "#             if col1==col2:\n",
    "#                 vif_df.loc[col1, col2]='und.'\n",
    "#             else:\n",
    "#                 vif_df.loc[col1, col2]=calculate_vif(temp, [col1, col2])\n",
    "\n",
    "#     vifs.append(vif_df)\n",
    "# # vif_df=vif_df.astype('float')\n",
    "\n",
    "# vifs[0] = vifs[0].drop(columns=['Rainfall'], index=['Rainfall'])\n",
    "\n",
    "# for season in range(3):\n",
    "#     for i in range(len(vifs[season])):\n",
    "#         for j in range(len(vifs[season])):\n",
    "#             if type(vifs[season].iloc[i, j])==np.float64:\n",
    "#                 vifs[season].iloc[i, j] = '%.3f' % vifs[season].iloc[i, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vifs=vifs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vifs=[]\n",
    "ind=0\n",
    "# for season in range(3):\n",
    "season=0\n",
    "for i in range(len(vifs[season])):\n",
    "    for j in range(len(vifs[season])):\n",
    "        if type(vifs[season].iloc[i, j])==np.float64:\n",
    "            if vifs[season].iloc[i, j]>5:\n",
    "                print(vifs[season].iloc[i, j])\n",
    "                final_vifs.append([vifs[season].index[i], vifs[season].columns[j], '%.3f' % vifs[season].iloc[i, j]])\n",
    "                # final_vifs.append([seasons[season], vifs[season].index[i], vifs[season].columns[j], '%.3f' % vifs[season].iloc[i, j]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "sns.reset_defaults()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'Arial',\n",
    "        'size': 30}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "seasons=['Summer', 'Fall', 'Spring']\n",
    "\n",
    "# matplotlib.rcParams.update({'font.size': 24})\n",
    "\n",
    "for season in range(3):\n",
    "    title_text = 'Collinearity Between Weather Factors (' + seasons[season]+')'\n",
    "    # Pop the headers from the data array\n",
    "    column_headers = list(vifs[season].columns)\n",
    "    row_headers = list(vifs[season].index)\n",
    "    # Get some lists of color specs for row and column headers\n",
    "#     rcolors = plt.cm.GnBu(np.full(len(row_headers), 0.6))\n",
    "#     ccolors = plt.cm.GnBu(np.full(len(column_headers), 0.6))\n",
    "    ax_color=['#f0f043' for i in range(len(row_headers))]\n",
    "    # Create the figure. Setting a small pad on tight_layout\n",
    "    # seems to better regulate white space. Sometimes experimenting\n",
    "    # with an explicit figsize here can produce better outcome.\n",
    "    plt.figure(linewidth=2,\n",
    "            edgecolor='black',\n",
    "            facecolor='white',\n",
    "            tight_layout={'pad':.5},\n",
    "            figsize=(8,3.5)\n",
    "            )\n",
    "    # Add a table at the bottom of the axes\n",
    "    table = plt.table(cellText=vifs[season].values,\n",
    "                        cellLoc='center',\n",
    "                        rowLabels=row_headers,\n",
    "                        rowColours=ax_color,\n",
    "                        rowLoc='center',\n",
    "                        colColours=ax_color,\n",
    "                        colLabels=column_headers,\n",
    "                        loc='center')\n",
    "    table.scale(1.75, 1.5)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row>=1:\n",
    "            # print(row, col)\n",
    "            if vifs[season].iloc[row-1, col]!='und.' and float(vifs[season].iloc[row-1, col])> 5 and col!=-1:\n",
    "                cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.box(on=None)\n",
    "    plt.title(title_text)\n",
    "    plt.draw()\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                'collinearity_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight', \n",
    "                    edgecolor=fig.get_edgecolor(), facecolor=fig.get_facecolor(),)\n",
    "    plt.show()\n",
    "    plt.cla()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_headers=[0, 1, 2, 3, 5, 6, 8]\n",
    "temp2 = pd.DataFrame(weather_np[0][:, curr_headers], columns=[\n",
    "                     i for i in weather_data.columns.tolist() if weather_data.columns.tolist().index(i) in curr_headers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns.tolist()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp2=temp.iloc[:, [0, 1, 2, 3, 5, 6, 8]]\n",
    "# temp2=temp.iloc[:, [0, 2, 3, 5, 6, 8]]\n",
    "# temp2=temp.iloc[:, [0, 2, 3, 6, 8]]\n",
    "import itertools\n",
    "curr_headers=[0, 1, 2, 3, 5, 6, 8]\n",
    "vif_3=[]\n",
    "    \n",
    "def calculate_vif(df, features, exp_df):    \n",
    "    # print(features)\n",
    "    for feature in features:\n",
    "        X = [f for f in features if f != feature]        \n",
    "        X, y = df[X], df[feature]\n",
    "        r2 = LinearRegression().fit(X, y).score(X, y)\n",
    "        vif = 1/(1-r2)\n",
    "\n",
    "        if vif>5:\n",
    "            df_temp=[x for x in features if x != feature]\n",
    "            df_temp.append(feature)\n",
    "            df_temp.append(vif)\n",
    "            exp_df=pd.concat([exp_df, pd.DataFrame(df_temp).transpose()], axis=0)\n",
    "            # print(' '.join([x for x in features if x != feature])+':', feature, vif)\n",
    "\n",
    "    return exp_df\n",
    "\n",
    "for season in range(3):\n",
    "    temp2 = pd.DataFrame(weather_np[season][:, curr_headers], columns=[\n",
    "                     i for i in weather_data.columns.tolist() if weather_data.columns.tolist().index(i) in curr_headers])\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "\n",
    "    for i in range(3, 7):\n",
    "        test=itertools.combinations(list(temp2.columns), i)\n",
    "\n",
    "        for i in test:\n",
    "            df = pd.concat(\n",
    "                [df, calculate_vif(temp2, list(i), pd.DataFrame())], ignore_index=True)\n",
    "            \n",
    "    vif_3.append(df)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[0] = vif_3[0].iloc[:3]\n",
    "vif_3[0] = vif_3[0].dropna(axis=1)\n",
    "vif_3[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[0]=vif_3[0].iloc[:3]\n",
    "vif_3[0]=vif_3[0].dropna(axis=1)\n",
    "vif_3[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[1]=vif_3[1].iloc[:3]\n",
    "vif_3[1]=vif_3[1].dropna(axis=1)\n",
    "vif_3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[1]=vif_3[1].iloc[:3]\n",
    "vif_3[1]=vif_3[1].dropna(axis=1)\n",
    "vif_3[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[2]=vif_3[2].iloc[:3]\n",
    "vif_3[2]=vif_3[2].dropna(axis=1)\n",
    "vif_3[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        vif_3[i].iloc[j][0] += ', '+vif_3[i].iloc[j][1]\n",
    "    # vif_3[i]=vif_3[i].drop(columns=[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):  \n",
    "    vif_3[i]=vif_3[i].drop(columns=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for j in range(len(vif_3[i])):\n",
    "        vif_3[i].iloc[j, 2]='%.3f' % vif_3[i].iloc[j, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_3[0].iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for season in range(3):\n",
    "season=0\n",
    "for i in range(len(vif_3[season])):\n",
    "    temp=[seasons[season]]\n",
    "    for j in vif_3[season].iloc[i].tolist():\n",
    "        temp.append(j)\n",
    "    final_vifs.append(temp[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vifs = [['Speed', 'Gust', '32.995'],\n",
    "              ['Gust', 'Speed', '32.995'],\n",
    "              ['UV', 'Solar', '296.449'],\n",
    "              ['Solar', 'UV', '296.449'],\n",
    "              ['Dew Point,\\nHumidity', 'Temp.', '31.926'],\n",
    "              ['Temperature,\\nHumidity', 'Dew Point', '8.362'],\n",
    "              ['Temperature,\\nDew Point', 'Humidity', '24.150']]\n",
    "final_vifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(final_vifs, columns=['Input', 'Output', 'VIF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (6, 4)}, style='white', font='Arial', font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text = 'High Collinearity Between Weather Factors (Summer)'\n",
    "# Pop the headers from the data array\n",
    "column_headers = ['Input', 'Output', 'VIF']\n",
    "# row_headers = list(vifs[season].index)\n",
    "# Get some lists of color specs for row and column headers\n",
    "#     rcolors = plt.cm.GnBu(np.full(len(row_headers), 0.6))\n",
    "#     ccolors = plt.cm.GnBu(np.full(len(column_headers), 0.6))\n",
    "ax_color=['#f0f043' for i in range(len(column_headers))]\n",
    "# Create the figure. Setting a small pad on tight_layout\n",
    "# seems to better regulate white space. Sometimes experimenting\n",
    "# with an explicit figsize here can produce better outcome.\n",
    "plt.figure(linewidth=2,\n",
    "        edgecolor='white',\n",
    "        facecolor='white',\n",
    "        tight_layout={'pad':.5},\n",
    "        figsize=(6,4.5)\n",
    "        )\n",
    "# Add a table at the bottom of the axes\n",
    "table = plt.table(cellText=x.values,\n",
    "                    cellLoc='center',\n",
    "                    # rowLabels=row_headers,\n",
    "                    # rowColours=ax_color,\n",
    "                    # rowLoc='center',\n",
    "                    colColours=ax_color,\n",
    "                    colLabels=column_headers,\n",
    "                    loc='center')\n",
    "table.scale(0.6, 2.5)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "\n",
    "# for (row, col), cell in table.get_celld().items():\n",
    "#     if row>=1 and col==2:\n",
    "#     #     print(row, col)\n",
    "#         cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "#     #     if vifs[season].iloc[row-1, col]!='und.' and float(vifs[season].iloc[row-1, col])> 5 and col!=-1:\n",
    "            \n",
    "\n",
    "ax = plt.gca()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.box(on=None)\n",
    "plt.title(title_text)\n",
    "plt.draw()\n",
    "fig = plt.gcf()\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'collinearity_summer_alt.jpg'), dpi=600, bbox_inches='tight', \n",
    "                edgecolor=fig.get_edgecolor(), facecolor=fig.get_facecolor(),)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    title_text = 'Multicollinearity Between Weather Factors (' + seasons[season]+')'\n",
    "    # Pop the headers from the data array\n",
    "    column_headers = ['Input', 'Output', 'VIF']\n",
    "    # row_headers = list(vifs[season].index)\n",
    "    # Get some lists of color specs for row and column headers\n",
    "    #     rcolors = plt.cm.GnBu(np.full(len(row_headers), 0.6))\n",
    "    #     ccolors = plt.cm.GnBu(np.full(len(column_headers), 0.6))\n",
    "    ax_color=['#f0f043' for i in range(len(row_headers))]\n",
    "    # Create the figure. Setting a small pad on tight_layout\n",
    "    # seems to better regulate white space. Sometimes experimenting\n",
    "    # with an explicit figsize here can produce better outcome.\n",
    "    plt.figure(linewidth=2,\n",
    "            edgecolor='black',\n",
    "            facecolor='white',\n",
    "            tight_layout={'pad':.5},\n",
    "            figsize=(6,2)\n",
    "            )\n",
    "    # Add a table at the bottom of the axes\n",
    "    table = plt.table(cellText=vif_3[season].values,\n",
    "                        cellLoc='center',\n",
    "                        # rowLabels=row_headers,\n",
    "                        # rowColours=ax_color,\n",
    "                        # rowLoc='center',\n",
    "                        colColours=ax_color,\n",
    "                        colLabels=column_headers,\n",
    "                        loc='center')\n",
    "    table.scale(1, 1.5)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row>=1 and col==2:\n",
    "        #     print(row, col)\n",
    "            cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "        #     if vifs[season].iloc[row-1, col]!='und.' and float(vifs[season].iloc[row-1, col])> 5 and col!=-1:\n",
    "                \n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.box(on=None)\n",
    "    plt.title(title_text)\n",
    "    plt.draw()\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                'multicollinearity_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight', \n",
    "                    edgecolor=fig.get_edgecolor(), facecolor=fig.get_facecolor(),)\n",
    "    plt.show()\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(directory, 'Figures', 'vif_multiple.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "tests=20\n",
    "# should be roughly 2 weeks\n",
    "initial_hours=336"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time lagging\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "\n",
    "compiled_metrics = []\n",
    "for season in range(3):\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "    compiled_metrics.append([])\n",
    "    for time_steps in all_time_steps:\n",
    "        index = 0\n",
    "        X2 = []\n",
    "        # Y2=Y.iloc[time_steps-1:]\n",
    "\n",
    "        print('time step', time_steps)\n",
    "\n",
    "        for date in data_blocks[season]:\n",
    "            for i in range(index, len(weather_data)):\n",
    "                if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                    temp = []\n",
    "                    # print('name', weather_data.iloc[i].name)\n",
    "                    for j in range(i, i+time_steps-1+24):\n",
    "                        temp.append(\n",
    "                            weather_data.iloc[j, weather_columns].values)\n",
    "                    for hour in range(24):\n",
    "                        X2.append(np.array(temp[hour:hour+time_steps]))\n",
    "                    index = i\n",
    "                    break\n",
    "        X2 = np.array(X2)\n",
    "\n",
    "        X = X2\n",
    "        # X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "\n",
    "        model_gru = Sequential([\n",
    "            GRU(16, activation='relu', return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.15),\n",
    "            GRU(16, activation='relu', input_shape=(\n",
    "                time_steps, len(weather_columns))),\n",
    "            Dropout(0.1),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "        model_lstm = Sequential([\n",
    "            LSTM(16, activation='relu', return_sequences=True,\n",
    "                 input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.15),\n",
    "            LSTM(16, activation='relu', input_shape=(\n",
    "                time_steps, len(weather_columns))),\n",
    "            Dropout(0.1),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "        metrics = [[] for i in range(4)]\n",
    "#         model_mse=[]\n",
    "#         model_r2=[]\n",
    "        tests = 10\n",
    "        kf = KFold(n_splits=tests)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            #             X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "            X_train = X[train_index]\n",
    "            X_test = X[test_index]\n",
    "            Y_train = Y.iloc[train_index]\n",
    "            Y_test = Y.iloc[test_index]\n",
    "\n",
    "            scalers = []\n",
    "            for i in range(X_train.shape[2]):\n",
    "                scalers.append(StandardScaler())\n",
    "                X_train[:, :, i] = scalers[i].fit_transform(X_train[:, :, i])\n",
    "            for i in range(X_test.shape[2]):\n",
    "                X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "            X_train = np.asarray(X_train).astype('float32')\n",
    "            X_test = np.asarray(X_test).astype('float32')\n",
    "            Y_train = np.asarray(Y_train).astype('float32')\n",
    "            Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "            model_lstm.fit(X_train, Y_train, epochs=10,\n",
    "                           batch_size=16, verbose=False)\n",
    "\n",
    "            y_pred = model_lstm.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "            mse = mean_squared_error(Y_test, y_pred)\n",
    "            metrics[0].append(mse)\n",
    "            r2 = r2_score(Y_test, y_pred)\n",
    "            metrics[1].append(r2)\n",
    "\n",
    "            model_gru.fit(X_train, Y_train, epochs=10,\n",
    "                          batch_size=16, verbose=False)\n",
    "\n",
    "            y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "            mse = mean_squared_error(Y_test, y_pred)\n",
    "            metrics[2].append(mse)\n",
    "            r2 = r2_score(Y_test, y_pred)\n",
    "            metrics[3].append(r2)\n",
    "\n",
    "        compiled_metrics[-1].append(pd.DataFrame(metrics).transpose())\n",
    "#         compiled_r2.append(pd.DataFrame(model_r2))\n",
    "#         compiled_mse.append(pd.DataFrame(model_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_df = [pd.DataFrame(columns=['LSTM MSE', 'GRU MSE',\n",
    "                             'LSTM R2', 'GRU R2'], index=all_time_steps) for i in range(3)]\n",
    "for season in range(3):\n",
    "    for i in range(12):\n",
    "        time_step_df[season].iloc[i, 0] = compiled_metrics[season][i][0].mean()\n",
    "        time_step_df[season].iloc[i, 1] = compiled_metrics[season][i][2].mean()\n",
    "        time_step_df[season].iloc[i, 2] = compiled_metrics[season][i][1].mean()\n",
    "        time_step_df[season].iloc[i, 3] = compiled_metrics[season][i][3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_steps=[1, 2, 3, 4, 6, 9, 12, 16, 20, 24, 48, 72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_df[0].iloc[9, 1]-=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)}, style='white', font='Arial', font_scale=1.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['Summer', 'Fall', 'Spring']\n",
    "for season in range(3):\n",
    "    indices = [i for i in range(len(all_time_steps))]\n",
    "    indexed = time_step_df[season].iloc[:, 0]\n",
    "    indexed.index = indices\n",
    "    indexed2 = time_step_df[season].iloc[:, 1]\n",
    "    indexed2.index = indices\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    ax.xaxis.set_ticks(indices)\n",
    "    ax.xaxis.set_ticklabels(all_time_steps)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "    indexed.plot(ax=ax, legend=True)\n",
    "    indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Time Step (Hours)')\n",
    "    plt.ylabel('R-Squared Score')\n",
    "    plt.title('Performance Across Time Steps (' +\n",
    "              seasons[season]+')')\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                'time_step_r2_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns[[0, 2, 3, 5, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subset_metrics=[]\n",
    "\n",
    "leave_out=[['Pressure'], ['Pressure', 'Rainfall'], ['Pressure', 'Rainfall', 'Speed']]\n",
    "\n",
    "for season in range(3):\n",
    "    subset_metrics.append([])\n",
    "    #k-fold cross validation\n",
    "    # neither precip nor barometric\n",
    "    # full_weather_columns = [[0, 2, 3, 5, 6, 8], [0, 2, 3, 5, 8], [0, 2, 3, 6, 8], [0, 2, 3, 8]]\n",
    "    # column_headers=[['LSTM-6', 'GRU-6'], ['LSTM-4+P', 'GRU-4+P'], ['LSTM-4+R', 'GRU-4+R'], ['LSTM-4', 'GRU-4']]\n",
    "    drop_combinations = []\n",
    "\n",
    "    for i in range(len(leave_out[season])+1):\n",
    "        for j in itertools.combinations(leave_out[season], i):\n",
    "            temp = []\n",
    "            for k in j:\n",
    "                temp.append(k)\n",
    "            drop_combinations.append(temp)\n",
    "\n",
    "    print(drop_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[weather_data.columns].index('Rainfall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "weather_columns=weather_data.columns.tolist()\n",
    "for factor in drop_combinations[i]:\n",
    "    del weather_columns[weather_columns.index(factor)]\n",
    "\n",
    "weather_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "time_steps = 9\n",
    "index=0\n",
    "\n",
    "weather_columns=[0, 2, 3, 8]\n",
    "\n",
    "for date in data_blocks[season]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name==date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp=[]\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index=i\n",
    "            break\n",
    "X=np.array(X)\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "for model_type in range(2):\n",
    "    r2_arr.append([])\n",
    "    tests=10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    row=[[], []]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "        # print(train_index)\n",
    "        # print(test_index)\n",
    "        # holdon = train_index\n",
    "\n",
    "        # print(X.shape, len(train_index))\n",
    "\n",
    "        if model_type==0:\n",
    "            model = Sequential([\n",
    "                LSTM(16, activation='relu', return_sequences=True,\n",
    "                    input_shape=(time_steps, len(weather_columns))),\n",
    "                Dropout(0.15),\n",
    "                LSTM(16, activation='relu',\n",
    "                    input_shape=(time_steps, len(weather_columns))),\n",
    "                Dropout(0.1),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            model = Sequential([\n",
    "                GRU(16, activation='relu', return_sequences=True,\n",
    "                    input_shape=(time_steps, len(weather_columns))),\n",
    "                Dropout(0.15),\n",
    "                GRU(16, activation='relu',\n",
    "                    input_shape=(time_steps, len(weather_columns))),\n",
    "                Dropout(0.1),\n",
    "                Dense(1)\n",
    "            ])\n",
    "\n",
    "        model.compile(metrics=['mse'], loss='mse')\n",
    "        \n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scalers=[]\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train[:, :, i]=scalers[i].fit_transform(X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i]=scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=10,\n",
    "                    batch_size=16, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        r2_arr[-1].append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[-2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "set_weather_columns=[0, 2, 3, 5, 6, 8]\n",
    "# optimal_time_steps = [[8, 6, 6], [12, 12, 10]]\n",
    "\n",
    "subset_metrics=[[], []]\n",
    "\n",
    "leave_out=[['Pressure', 'Rainfall'], ['Pressure', 'Rainfall'], ['Pressure', 'Rainfall']]\n",
    "models=['LSTM', 'GRU']\n",
    "\n",
    "for model_type in range(2):\n",
    "    for season in range(3):\n",
    "        print('season', season)\n",
    "        subset_metrics[0].append([])\n",
    "        subset_metrics[1].append([])\n",
    "        #k-fold cross validation\n",
    "        # neither precip nor barometric\n",
    "        # full_weather_columns = [[0, 2, 3, 5, 6, 8], [0, 2, 3, 5, 8], [0, 2, 3, 6, 8], [0, 2, 3, 8]]\n",
    "        # column_headers=[['LSTM-6', 'GRU-6'], ['LSTM-4+P', 'GRU-4+P'], ['LSTM-4+R', 'GRU-4+R'], ['LSTM-4', 'GRU-4']\n",
    "    \n",
    "        weather_columns=[i for i in range(len(weather_data.columns)-1)]\n",
    "        # weather_columns=set_weather_columns\n",
    "        X=[]\n",
    "        time_steps = 9\n",
    "        index=0\n",
    "\n",
    "        for date in data_blocks[season]:\n",
    "            for i in range(index, len(weather_data)):\n",
    "                if weather_data.iloc[i].name==date-pd.Timedelta(hours=time_steps-1):\n",
    "                    temp=[]\n",
    "                    # print('name', weather_data.iloc[i].name)\n",
    "                    for j in range(i, i+time_steps+24):\n",
    "                        temp.append(weather_data.iloc[j, weather_columns].values)\n",
    "                    for hour in range(24):\n",
    "                        X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                    index=i\n",
    "                    break\n",
    "        X_orig=np.array(X)\n",
    "        Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "        drop_combinations=[]\n",
    "\n",
    "        for i in range(len(leave_out[season])+1):\n",
    "            for j in itertools.combinations(leave_out[season], i):\n",
    "                temp = []\n",
    "                for k in j:\n",
    "                    temp.append(k)\n",
    "                drop_combinations.append(temp)\n",
    "\n",
    "        print(drop_combinations)\n",
    "                \n",
    "        df=[pd.DataFrame() for i in range(2)]\n",
    "        \n",
    "        for i_drop in range(len(drop_combinations)):\n",
    "            # weather_columns=[i for i in range(len(weather_data.columns)-1)]\n",
    "            weather_columns=set_weather_columns.copy()\n",
    "            # temp_remove=[]\n",
    "            # print(drop_combinations[i])\n",
    "            print(drop_combinations[i_drop])\n",
    "            for factor in drop_combinations[i_drop]:\n",
    "                print(weather_data.columns.tolist().index(factor))\n",
    "                temp_int=weather_data.columns.tolist().index(factor)\n",
    "                if temp_int in weather_columns:\n",
    "                    weather_columns.remove(temp_int)\n",
    "                print(weather_columns)\n",
    "            # for j in temp_remove:\n",
    "            #     for k in range(len())\n",
    "\n",
    "            # print(drop_combinations[i])\n",
    "            # print(weather_columns)\n",
    "\n",
    "            X=X_orig[:, :, weather_columns]\n",
    "\n",
    "            # print(X.shape)\n",
    "            \n",
    "            tests=10\n",
    "            kf = KFold(n_splits=tests)\n",
    "\n",
    "            row=[[], []]\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "                # print(train_index)\n",
    "                # print(test_index)\n",
    "                # holdon = train_index\n",
    "\n",
    "                # print(X.shape, len(train_index))\n",
    "\n",
    "                if model_type==0:\n",
    "                    model = Sequential([\n",
    "                        LSTM(16, return_sequences=True,\n",
    "                            input_shape=(time_steps, len(weather_columns))),\n",
    "                        Dropout(0.2),\n",
    "                        LSTM(16,\n",
    "                            input_shape=(time_steps, len(weather_columns))),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "\n",
    "                else:\n",
    "                    model = Sequential([\n",
    "                        GRU(16, return_sequences=True,\n",
    "                            input_shape=(time_steps, len(weather_columns))),\n",
    "                        Dropout(0.2),\n",
    "                        GRU(16,\n",
    "                            input_shape=(time_steps, len(weather_columns))),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "\n",
    "                model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "                \n",
    "                X_train = X[train_index]\n",
    "                X_test = X[test_index]\n",
    "                Y_train = Y.iloc[train_index]\n",
    "                Y_test = Y.iloc[test_index]\n",
    "\n",
    "                X_train = np.asarray(X_train).astype('float32')\n",
    "                X_test = np.asarray(X_test).astype('float32')\n",
    "                Y_train = np.asarray(Y_train).astype('float32')\n",
    "                Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "                scalers=[]\n",
    "                for i in range(X_train.shape[2]):\n",
    "                    scalers.append(StandardScaler())\n",
    "                    X_train[:, :, i]=scalers[i].fit_transform(X_train[:, :, i])\n",
    "                for i in range(X_test.shape[2]):\n",
    "                    X_test[:, :, i]=scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "                model.fit(X_train, Y_train, epochs=10,\n",
    "                            batch_size=16, verbose=0)\n",
    "                \n",
    "                y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "                mse = mean_squared_error(Y_test, y_pred)\n",
    "                row[0].append(mse)\n",
    "                r2 = r2_score(Y_test, y_pred)\n",
    "                row[1].append(r2)\n",
    "                # row_name = models[model_type]\n",
    "                # for factor in drop_combinations[i_drop]:\n",
    "                #      row_name+='-'+factor[:1]\n",
    "\n",
    "            row_name = models[model_type]\n",
    "            if len(drop_combinations[i_drop])>=1:\n",
    "                row_name+='-'\n",
    "            row_name += '-'.join([factor[:1] for factor in drop_combinations[i_drop]])\n",
    "            df[0][row_name]=row[0]\n",
    "            df[1][row_name]=row[1]\n",
    "            print(row_name)\n",
    "\n",
    "        subset_metrics[0][-1].append(df[0].transpose())\n",
    "        subset_metrics[1][-1].append(df[1].transpose())\n",
    "                # r2 = r2_score(Y_test, y_pred)\n",
    "                # model_r2.append(r2)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_subset_metrics=subset_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_subset_metrics[1][2][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    for ind in subset_metrics[1][season][0].index.tolist():\n",
    "        print(seasons[season], ind, subset_metrics[1][season][0].loc[ind].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    for ind in subset_metrics[0][season][0].index.tolist():\n",
    "        print(seasons[season], ind, subset_metrics[1][season][0].loc[ind].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new subset figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season_ind={'Summer': 0, 'Winter': 1, 'Spring': 2}\n",
    "subsets=[[[] for i in range(3)],  [[] for i in range(3)]]\n",
    "for x in range(2):\n",
    "    for season in range(x*3, x*3+3):\n",
    "        for ind in saved_subset_metrics[1][season][0].index.tolist():\n",
    "            subsets[x][season%3].append(saved_subset_metrics[1]\n",
    "                [season][0].loc[ind].mean())\n",
    "            # print(seasons[season], ind, subset_metrics[1]\n",
    "            #     [season][0].loc[ind].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bplot=[]\n",
    "dropped_factors=['Pressure', 'Rainfall', 'Pressure and Rainfall']\n",
    "for x in range(2):\n",
    "    for season in range(3):\n",
    "        for i in range(1, 4):\n",
    "            temp=[]\n",
    "            temp.append(subsets[x][season][i]-subsets[x][season][0])\n",
    "            temp.append(dropped_factors[i-1])\n",
    "            temp.append(seasons[season])\n",
    "            subset_bplot.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df=pd.DataFrame(subset_bplot, columns=['Difference', 'Weather Factors', 'Season'])\n",
    "subset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arr = [[[], [], []], [[], [], []]]\n",
    "for i in range(int(len(subset_df)/2)):\n",
    "    plot_arr[0][int(i % 3)].append(subset_df.iloc[i, 0])\n",
    "for i in range(int(len(subset_df)/2), len(subset_df)):\n",
    "    plot_arr[1][int(i % 3)].append(subset_df.iloc[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "matplotlib.rc('xtick', labelsize=20)\n",
    "matplotlib.rc('ytick', labelsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arr[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_plot_arr=plot_arr\n",
    "saved_plot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = seasons\n",
    "bar_width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "index = np.arange(len(labels))\n",
    "rects1 = ax.bar(index, plot_arr[0][0], bar_width, color='r', label='Pressure')\n",
    "rects2 = ax.bar(index + bar_width, plot_arr[0][1], bar_width, color='b', label='Rainfall')\n",
    "rects3 = ax.bar(index + bar_width*2, plot_arr[0][2], bar_width, color='g', label='Pressure and Rainfall')\n",
    "\n",
    "# ax.set_xlabel('Dropped Weather Factor')\n",
    "ax.set_ylabel('Average R-Squared Score Gain')\n",
    "ax.set_title('Average Gain From Dropping Weather Factors (LSTM)')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'feature_gain_lstm.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = seasons\n",
    "bar_width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "index = np.arange(len(labels))\n",
    "rects1 = ax.bar(index, plot_arr[1][0], bar_width, color='r', label='Pressure')\n",
    "rects2 = ax.bar(index + bar_width, plot_arr[1][1], bar_width, color='b', label='Rainfall')\n",
    "rects3 = ax.bar(index + bar_width*2, plot_arr[1][2], bar_width, color='g', label='Pressure and Rainfall')\n",
    "\n",
    "# ax.set_xlabel('Dropped Weather Factor')\n",
    "ax.set_ylabel('Average R-Squared Score Gain')\n",
    "ax.set_title('Average Gain From Dropping Weather Factors (GRU)')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'feature_gain_gru.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = seasons\n",
    "bar_width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "index = np.arange(len(labels))\n",
    "rects1 = ax.bar(index, plot_arr[0], bar_width, color='r', label='Pressure')\n",
    "rects2 = ax.bar(index + bar_width, plot_arr[1], bar_width, color='b', label='Rainfall')\n",
    "rects3 = ax.bar(index + bar_width*2, plot_arr[2], bar_width, color='g', label='Pressure and Rainfall')\n",
    "\n",
    "# ax.set_xlabel('Dropped Weather Factor')\n",
    "ax.set_ylabel('Average R-Squared Score Gain')\n",
    "ax.set_title('Average Gain From Dropping Weather Factors')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "                         'feature_gain.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3, 6):\n",
    "    for ind in subset_metrics[1][season][0].index.tolist():\n",
    "        print(seasons[season-3], ind, subset_metrics[1]\n",
    "              [season][0].loc[ind].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse, then r2\n",
    "metrics=['Mean Squared Error', 'R-Squared Score']\n",
    "metrics_abv=['mse', 'r2']\n",
    "for metric in range(2):\n",
    "    for season in range(3):\n",
    "        curr_df=subset_metrics[metric][season][0].transpose()\n",
    "        sns.set_theme(rc={'figure.figsize': (2+len(curr_df.columns), 6)}, style='white', font='Arial', font_scale=1.2)\n",
    "\n",
    "        sns.boxplot(data=curr_df, color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "        plt.title('Model Accuracies on Feature Subsets ('+seasons[season]+')')\n",
    "        plt.ylabel('Model')\n",
    "        plt.ylabel(metrics[metric])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(os.path.join(directory, 'Figures',\n",
    "                    '_'.join(['features_lstm', seasons[season], metrics_abv[metric]])), dpi=600, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse, then r2\n",
    "# metrics=['Mean Squared Error', 'R-Squared Score']\n",
    "# metrics_abv=['mse', 'r2']\n",
    "# for metric in range(2):x\n",
    "metric=1\n",
    "for season in range(3):\n",
    "    curr_df=subset_metrics[metric][season][0].transpose()\n",
    "    sns.set_theme(rc={'figure.figsize': (8, 8)}, style='white', font='Arial', font_scale=1.5)\n",
    "\n",
    "    sns.boxplot(data=curr_df, color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "    plt.title('Model R-Squared Scores on Feature Subsets ('+seasons[season]+')')\n",
    "    plt.ylabel('Model')\n",
    "    plt.ylabel('R-Squared Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                '_'.join(['features_lstm', seasons[season], 'r2'])), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse, then r2\n",
    "# metrics=['Mean Squared Error', 'R-Squared Score']\n",
    "# metrics_abv=['mse', 'r2']\n",
    "# for metric in range(2):x\n",
    "metric=1\n",
    "for season in range(3):\n",
    "    curr_df=subset_metrics[metric][season][0].transpose()\n",
    "    sns.set_theme(rc={'figure.figsize': (8, 8)}, style='white', font='Arial', font_scale=1.5)\n",
    "\n",
    "    sns.boxplot(data=curr_df, color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "    plt.title('Model R-Squared Scores on Feature Subsets ('+seasons[season]+')')\n",
    "    plt.ylabel('Model')\n",
    "    plt.ylabel('R-Squared Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                '_'.join(['features_lstm', seasons[season], 'r2'])), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=1\n",
    "for season in range(3, 6):\n",
    "    curr_df=subset_metrics[metric][season][0].transpose()\n",
    "    # sns.set_theme(rc={'figure.figsize': (2+len(curr_df.columns), 6)}, style='white', font='Arial', font_scale=1.5)\n",
    "    sns.set_theme(rc={'figure.figsize': (8, 8)}, style='white', font='Arial', font_scale=1.5)\n",
    "    plt.title('Correlation of Weather and Foraging Activity Factors (Fall)')\n",
    "\n",
    "    sns.boxplot(data=curr_df, color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "    plt.title('Model R-Squared Scores on Feature Subsets ('+seasons[season-3]+')')\n",
    "    plt.ylabel('Model')\n",
    "    plt.ylabel('R-Squared Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                '_'.join(['features_gru', seasons[season-3], 'r2'])), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3, 6):\n",
    "    for ind in subset_metrics[1][season][0].index.tolist():\n",
    "        print(seasons[season-3], ind, subset_metrics[1]\n",
    "              [season][0].loc[ind].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 5)}, style='white', font='Arial', font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summer LSTM; Exclude pressure\n",
    "# Fall LSTM: Exclude rainfall\n",
    "# Spring LSTM: Exclude pressure and rainfall\n",
    "# Summer GRU; Exclude pressure\n",
    "# Fall GRU: Exclude rainfall\n",
    "# Spring GRU: Exclude rainfall\n",
    "\n",
    "seasons=['Summer', 'Fall', 'Spring']\n",
    "table_df = [['Summer', 'LSTM', 'Rainfall'], ['Summer', 'GRU', 'None'], \n",
    "            ['Fall', 'LSTM', 'None'], ['Fall', 'GRU', 'Pressure and\\nrainfall'], \n",
    "            ['Spring', 'LSTM', 'Pressure'], ['Spring', 'GRU', 'Pressure'], ]\n",
    "\n",
    "table_df=pd.DataFrame(table_df, columns=['Season', 'Model', 'Excluded\\nFactors'])\n",
    "\n",
    "# matplotlib.rcParams.update({'font.size': 24})\n",
    "\n",
    "title_text = 'Excluded Weather Factors'\n",
    "# Pop the headers from the data array\n",
    "column_headers = list(table_df.columns)\n",
    "# row_headers = list(table_df.index)\n",
    "# Get some lists of color specs for row and column headers\n",
    "#     rcolors = plt.cm.GnBu(np.full(len(row_headers), 0.6))\n",
    "#     ccolors = plt.cm.GnBu(np.full(len(column_headers), 0.6))\n",
    "ax_color = ['#f0f043' for i in range(len(column_headers))]\n",
    "# Create the figure. Setting a small pad on tight_layout\n",
    "# seems to better regulate white space. Sometimes experimenting\n",
    "# with an explicit figsize here can produce better outcome.\n",
    "plt.figure(linewidth=50,\n",
    "        # edgecolor='black',\n",
    "        facecolor='white',\n",
    "        tight_layout={'pad':.5},\n",
    "        figsize=(20,18.75)\n",
    "        )\n",
    "# Add a table at the bottom of the axes\n",
    "table = plt.table(cellText=table_df.values,\n",
    "                    cellLoc='center',\n",
    "                    # rowLabels=row_headers,\n",
    "                    # rowColours=ax_color,\n",
    "                    # rowLoc='center',\n",
    "                    colColours=ax_color,\n",
    "                    colLabels=column_headers,\n",
    "                    loc='center')\n",
    "table.scale(1.3, 10)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(60)\n",
    "\n",
    "# for (row, col), cell in table.get_celld().items():\n",
    "#     if row>=1:\n",
    "#         # print(row, col)\n",
    "#         if vifs[season].iloc[row-1, col]!='und.' and float(vifs[season].iloc[row-1, col])> 5 and col!=-1:\n",
    "#             cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.box(on=None)\n",
    "plt.title(title_text)\n",
    "plt.draw()\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'excluded_factors.jpg'), format='jpg', dpi=1200, bbox_inches='tight', \n",
    "                edgecolor=fig.get_edgecolor(), facecolor=fig.get_facecolor(),)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['Mean Squared Error', 'R-Squared Score']\n",
    "metrics_abv=['mse', 'r2']\n",
    "for metric in range(2):\n",
    "    for season in range(3, 6):\n",
    "        curr_df=subset_metrics[metric][season][0].transpose()\n",
    "        sns.set_theme(rc={'figure.figsize': (2+len(curr_df.columns), 6)}, style='white', font='Arial', font_scale=1.2)\n",
    "\n",
    "        sns.boxplot(data=curr_df, color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "        plt.title('Model Accuracies on Feature Subsets ('+seasons[season-3]+')')\n",
    "        plt.ylabel('Model')\n",
    "        plt.ylabel(metrics[metric])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.savefig(os.path.join(directory, 'Figures',\n",
    "                    '_'.join(['features_gru', seasons[season-3], metrics_abv[metric]])), dpi=600, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df[0].transpose().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_metrics[0][3][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_metrics[0][3][0][saved_df[0].transpose().index[0]]=saved_df[0].transpose().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_metrics[1][0][0].loc['LSTM'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_metrics[0][3].append(saved_df[0].transpose())\n",
    "subset_metrics[1][3].append(saved_df[1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(Y_test), pd.DataFrame(y_pred)], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm, then gru | summer, fall, spring\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "from tensorflow.keras.layers import Activation\n",
    "optimal_time_steps = [[8, 6, 6], [12, 12, 10]]\n",
    "optimal_features = [[0, 2, 3, 5, 6, 8], [0, 2, 3, 5, 6, 8], [0, 2, 8]]\n",
    "\n",
    "# bayesian optimization\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = [\n",
    "    Integer(4, 128, name='n_nodes'),\n",
    "    Integer(1, 3, name='n_layers'),\n",
    "    Integer(4, 256, name='batch_size'),\n",
    "    Integer(3, 50, name='n_epochs'),\n",
    "    Real(1e-6, 1e-2, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.1, 0.5, name='dropout_rate'),\n",
    "    Categorical(['adam', 'sgd', 'rmsprop'], name='optimizer'),\n",
    "    Categorical(['relu', 'tanh', 'sigmoid'], name='activation')\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_type in range(2):\n",
    "    for season in range(3):\n",
    "        Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "        index = 0\n",
    "        X = []\n",
    "\n",
    "        time_steps = optimal_time_steps[model_type][season]\n",
    "        tests = 5\n",
    "\n",
    "        weather_columns = optimal_features[season]\n",
    "\n",
    "        for date in data_blocks[season]:\n",
    "            for i in range(index, len(weather_data)):\n",
    "                if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                    temp = []\n",
    "                    # print('name', weather_data.iloc[i].name)\n",
    "                    for j in range(i, i+time_steps-1+24):\n",
    "                        temp.append(\n",
    "                            weather_data.iloc[j, weather_columns].values)\n",
    "                    for hour in range(24):\n",
    "                        X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                    index = i\n",
    "                    break\n",
    "        X = np.array(X)\n",
    "\n",
    "        def make_model(n_nodes, n_layers, batch_size, learning_rate, dropout_rate, optimizer, activation):\n",
    "            model = Sequential()\n",
    "            if n_layers == 1:\n",
    "                if model_type == 0:\n",
    "                    model.add(LSTM(units=n_nodes, activation=activation,\n",
    "                              input_shape=(time_steps, len(weather_columns))))\n",
    "                else:\n",
    "                    model.add(GRU(units=n_nodes, activation=activation,\n",
    "                              input_shape=(time_steps, len(weather_columns))))\n",
    "            else:\n",
    "                for i in range(n_layers):\n",
    "                    if i == 0:\n",
    "                        if model_type == 0:\n",
    "                            model.add(LSTM(units=n_nodes, activation=activation, return_sequences=True, input_shape=(\n",
    "                                time_steps, len(weather_columns))))\n",
    "                        else:\n",
    "                            model.add(GRU(units=n_nodes, activation=activation, return_sequences=True, input_shape=(\n",
    "                                time_steps, len(weather_columns))))\n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                    elif i == n_layers - 1:\n",
    "                        if model_type == 0:\n",
    "                            model.add(\n",
    "                                LSTM(units=n_nodes, activation=activation))\n",
    "                        else:\n",
    "                            if model_type == 0:\n",
    "                                model.add(\n",
    "                                    LSTM(units=n_nodes, activation=activation))\n",
    "                            else:\n",
    "                                model.add(\n",
    "                                    GRU(units=n_nodes, activation=activation))\n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                    else:\n",
    "                        model.add(\n",
    "                            GRU(units=n_nodes, activation=activation, return_sequences=True))\n",
    "                        model.add(Dropout(dropout_rate))\n",
    "            model.add(Dense(units=1))\n",
    "            if optimizer == 'adam':\n",
    "                optimizer = tf.keras.optimizers.Adam(\n",
    "                    learning_rate=learning_rate)\n",
    "            elif optimizer == 'sgd':\n",
    "                optimizer = tf.keras.optimizers.SGD(\n",
    "                    learning_rate=learning_rate)\n",
    "            elif optimizer == 'rmsprop':\n",
    "                optimizer = tf.keras.optimizers.RMSprop(\n",
    "                    learning_rate=learning_rate)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "            return model\n",
    "\n",
    "        # Define the objective function to minimize (negative mean squared error)\n",
    "        @use_named_args(space)\n",
    "        def objective(n_nodes, n_layers, batch_size, learning_rate, dropout_rate, optimizer, activation, n_epochs):\n",
    "            # Create the LSTM model\n",
    "            model = make_model(n_nodes=n_nodes, n_layers=n_layers, batch_size=batch_size,\n",
    "                               learning_rate=learning_rate, dropout_rate=dropout_rate, optimizer=optimizer, activation=activation)\n",
    "            # Train the model on the training data\n",
    "\n",
    "            kf = KFold(n_splits=tests)\n",
    "            mse = 0\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train = X[train_index]\n",
    "                X_test = X[test_index]\n",
    "                Y_train = Y.iloc[train_index]\n",
    "                Y_test = Y.iloc[test_index]\n",
    "\n",
    "                X_train = np.asarray(X_train).astype('float32')\n",
    "                X_test = np.asarray(X_test).astype('float32')\n",
    "                Y_train = np.asarray(Y_train).astype('float32')\n",
    "                Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "                scalers = []\n",
    "                for i in range(X_train.shape[2]):\n",
    "                    scalers.append(StandardScaler())\n",
    "                    X_train[:, :, i] = scalers[i].fit_transform(\n",
    "                        X_train[:, :, i])\n",
    "                for i in range(X_test.shape[2]):\n",
    "                    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "                model.fit(X_train, Y_train, batch_size=batch_size,\n",
    "                          epochs=n_epochs, verbose=0)\n",
    "                y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "                mse += mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "            mse /= tests\n",
    "            print(mse, n_nodes, n_layers, batch_size, learning_rate,\n",
    "                  dropout_rate, optimizer, activation, n_epochs)\n",
    "            return mse\n",
    "\n",
    "        # Specify the checkpoint saven_nodes, n_layers, batch_size, learning_rate, dropout_rate, optimizer, activation, n_epochsr to save the best model during optimization\n",
    "        checkpoint_saver = CheckpointSaver(os.path.join(\n",
    "            directory, 'best_model_'+str(model_type)+'_'+seasons[season]+'.h5'), compress=9)\n",
    "\n",
    "        # Run the Bayesian optimization\n",
    "        result = gp_minimize(objective, space, n_calls=50,\n",
    "                             n_random_starts=5, verbose=1, callback=[checkpoint_saver])\n",
    "\n",
    "        all_results.append(result)\n",
    "\n",
    "        print(\"Best hyperparameters: \", model_type, season, result.x)\n",
    "        print(\"Best mean squared error: \", model_type, season, result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# plot_df=pd.DataFrame(columns=['Actual Bees Outside', 'Predicted Bees Outside'])\n",
    "plot_df=pd.DataFrame()\n",
    "\n",
    "weather_columns = [i for i in range(len(weather_data.columns)-1)]\n",
    "# weather_columns=[0, 2, 8]\n",
    "X = []\n",
    "season = 0\n",
    "time_steps = 9\n",
    "index = 0\n",
    "\n",
    "for date in data_blocks[season]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X = np.array(X)\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# drop_combinations = []\n",
    "\n",
    "# for i in range(len(leave_out[season])+1):\n",
    "#     for j in itertools.combinations(leave_out[season], i):\n",
    "#         temp = []\n",
    "#         for k in j:\n",
    "#             temp.append(k)\n",
    "#         drop_combinations.append(temp)\n",
    "\n",
    "# printdrop_combinations)\n",
    "\n",
    "\n",
    "tests = 10\n",
    "kf = KFold(n_splits=tests)\n",
    "\n",
    "row = [[], []]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    model = Sequential([\n",
    "        GRU(8, activation='relu', return_sequences=True,\n",
    "            input_shape=(time_steps, len(weather_columns))),\n",
    "        Dropout(0.2),\n",
    "        GRU(8, activation='relu',\n",
    "            input_shape=(time_steps, len(weather_columns))),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scalers = []\n",
    "    for i in range(X_train.shape[2]):\n",
    "        scalers.append(StandardScaler())\n",
    "        X_train[:, :, i] = scalers[i].fit_transform(\n",
    "            X_train[:, :, i])\n",
    "    for i in range(X_test.shape[2]):\n",
    "        X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100,\n",
    "                batch_size=8, verbose=1)\n",
    "\n",
    "    y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "    combined=pd.concat([pd.DataFrame(Y_test), pd.DataFrame(y_pred)], axis=1)\n",
    "    plot_df = pd.concat([plot_df, combined], axis=0, ignore_index=True)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    row[0].append(mse)\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    row[1].append(r2)\n",
    "    # row_name = models[model_type]\n",
    "    # for factor in drop_combinations[i_drop]:\n",
    "    #      row_name+='-'+factor[:1]\n",
    "\n",
    "# row_name = models[model_type]\n",
    "# if len(drop_combinations[i_drop]) >= 1:\n",
    "#     row_name += '-'\n",
    "# row_name += '-'.join([factor[:1]\n",
    "#                         for factor in drop_combinations[i_drop]])\n",
    "# df[0][row_name] = row[0]\n",
    "# df[1][row_name] = row[1]\n",
    "# print(row_name)\n",
    "\n",
    "# subset_metrics[0][-1].append(df[0].transpose())\n",
    "# subset_metrics[1][-1].append(df[1].transpose())\n",
    "# r2 = r2_score(Y_test, y_pred)\n",
    "        # model_r2.append(r2)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[0]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_arr=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final accuracy validation\n",
    "\n",
    "plot_df = [pd.DataFrame() for i in range(3)]\n",
    "\n",
    "optimal_features=[[0, 2, 3, 5, 6, 8], [0, 2, 3, 8], [0, 2, 3, 6, 8]]\n",
    "\n",
    "# weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "# weather_columns=[0, 2, 8]\n",
    "model_type = 1\n",
    "# season = 0\n",
    "time_steps = 9\n",
    "\n",
    "for season in range(3):\n",
    "    index = 0\n",
    "    X = []\n",
    "\n",
    "    weather_columns=optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    # row = [[], []]\n",
    "    r2_arr.append([])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50,\n",
    "                batch_size=16, verbose=1)\n",
    "\n",
    "        y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred[i, 0]=max(y_pred[i, 0], 0)\n",
    "\n",
    "        combined = pd.concat([pd.DataFrame(Y_test), pd.DataFrame(y_pred)], axis=1)\n",
    "        plot_df[season] = pd.concat(\n",
    "            [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        # row[1].append(r2)\n",
    "        r2_arr[-1].append(r2)\n",
    "\n",
    "    # print(pd.DataFrame(row[1]).mean())\n",
    "    # row_name = models[model_type]\n",
    "    # for factor in drop_combinations[i_drop]:\n",
    "    #      row_name+='-'+factor[:1]\n",
    "\n",
    "# row_name = models[model_type]\n",
    "# if len(drop_combinations[i_drop]) >= 1:\n",
    "#     row_name += '-'\n",
    "# row_name += '-'.join([factor[:1]\n",
    "#                         for factor in drop_combinations[i_drop]])\n",
    "# df[0][row_name] = row[0]\n",
    "# df[1][row_name] = row[1]\n",
    "# print(row_name)\n",
    "\n",
    "# subset_metrics[0][-1].append(df[0].transpose())\n",
    "# subset_metrics[1][-1].append(df[1].transpose())\n",
    "# r2 = r2_score(Y_test, y_pred)\n",
    "    # model_r2.append(r2)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns.tolist()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM final accuracy validation\n",
    "\n",
    "plot_df = [pd.DataFrame() for i in range(3)]\n",
    "\n",
    "optimal_features=[[0, 2, 3, 5, 8], [0, 2, 3,5,6, 8], [0, 2, 3, 6, 8]]\n",
    "\n",
    "# weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "# weather_columns=[0, 2, 8]\n",
    "model_type = 0\n",
    "# season = 0\n",
    "time_steps = 9\n",
    "\n",
    "for season in range(3):\n",
    "    index = 0\n",
    "    X = []\n",
    "\n",
    "    weather_columns=optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    # row = [[], []]\n",
    "    r2_arr.append([])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            LSTM(64),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=75,\n",
    "                batch_size=8, verbose=1)\n",
    "\n",
    "        y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred[i, 0]=max(y_pred[i, 0], 0)\n",
    "\n",
    "        combined = pd.concat([pd.DataFrame(Y_test), pd.DataFrame(y_pred)], axis=1)\n",
    "        plot_df[season] = pd.concat(\n",
    "            [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        # row[1].append(r2)\n",
    "        r2_arr[-1].append(r2)\n",
    "\n",
    "    # print(pd.DataFrame(row[1]).mean())\n",
    "    # row_name = models[model_type]\n",
    "    # for factor in drop_combinations[i_drop]:\n",
    "    #      row_name+='-'+factor[:1]\n",
    "\n",
    "# row_name = models[model_type]\n",
    "# if len(drop_combinations[i_drop]) >= 1:\n",
    "#     row_name += '-'\n",
    "# row_name += '-'.join([factor[:1]\n",
    "#                         for factor in drop_combinations[i_drop]])\n",
    "# df[0][row_name] = row[0]\n",
    "# df[1][row_name] = row[1]\n",
    "# print(row_name)\n",
    "\n",
    "# subset_metrics[0][-1].append(df[0].transpose())\n",
    "# subset_metrics[1][-1].append(df[1].transpose())\n",
    "# r2 = r2_score(Y_test, y_pred)\n",
    "    # model_r2.append(r2)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 7):\n",
    "    print(pd.DataFrame(r2_arr[-i]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    print(pd.DataFrame(r2_arr[-i]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    print(pd.DataFrame(r2_arr[-i]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    print(pd.DataFrame(r2_arr[-i]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_plot_df=plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='white', font='Arial', font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summer LSTM; Exclude pressure\n",
    "# Fall LSTM: Exclude rainfall\n",
    "# Spring LSTM: Exclude pressure and rainfall\n",
    "# Summer GRU; Exclude pressure\n",
    "# Fall GRU: Exclude rainfall\n",
    "# Spring GRU: Exclude rainfall\n",
    "table_df = [['Summer', '0.86', '0.91'], \n",
    "            ['Fall', '0.83', '0.89'], \n",
    "            ['Spring', '0.71', '0.84']]\n",
    "\n",
    "table_df = pd.DataFrame(\n",
    "    table_df, columns=['Season', 'Initial R2\\nScore', 'Final R2\\nScore'])\n",
    "title_text=\"Improvement Through Model Optimization\"\n",
    "\n",
    "# matplotlib.rcParams.update({'font.size': 24})\n",
    "\n",
    "# title_text = 'Model Improvement Through Optimization'\n",
    "# Pop the headers from the data array\n",
    "column_headers = list(table_df.columns)\n",
    "# row_headers = list(table_df.index)\n",
    "# Get some lists of color specs for row and column headers\n",
    "#     rcolors = plt.cm.GnBu(np.full(len(row_headers), 0.6))\n",
    "#     ccolors = plt.cm.GnBu(np.full(len(column_headers), 0.6))\n",
    "ax_color = ['#f0f043' for i in range(len(column_headers))]\n",
    "# Create the figure. Setting a small pad on tight_layout\n",
    "# seems to better regulate white space. Sometimes experimenting\n",
    "# with an explicit figsize here can produce better outcome.\n",
    "plt.figure(linewidth=2,\n",
    "        edgecolor='black',\n",
    "        facecolor='white',\n",
    "        tight_layout={'pad':.5},\n",
    "        figsize=(3,3)\n",
    "        )\n",
    "# Add a table at the bottom of the axes\n",
    "table = plt.table(cellText=table_df.values,\n",
    "                    cellLoc='center',\n",
    "                    # rowLabels=row_headers,\n",
    "                    # rowColours=ax_color,\n",
    "                    # rowLoc='center',\n",
    "                    colColours=ax_color,\n",
    "                    colLabels=column_headers,\n",
    "                    loc='center')\n",
    "table.scale(1, 3)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "\n",
    "# for (row, col), cell in table.get_celld().items():\n",
    "#     if row>=1:\n",
    "#         # print(row, col)\n",
    "#         if vifs[season].iloc[row-1, col]!='und.' and float(vifs[season].iloc[row-1, col])> 5 and col!=-1:\n",
    "#             cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.box(on=None)\n",
    "plt.title(title_text)\n",
    "plt.draw()\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'model_improvement.jpg'), dpi=600, bbox_inches='tight', \n",
    "                edgecolor=fig.get_edgecolor(), facecolor=fig.get_facecolor(),)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[pd.to_datetime(\"2023-01-29\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    max_val=0\n",
    "    for i in range(len(saved_plot_df[season])):\n",
    "        for j in range(2):\n",
    "            if saved_plot_df[season].iloc[i, j]>max_val:\n",
    "                max_val=saved_plot_df[season].iloc[i, j]\n",
    "\n",
    "    saved_plot_df[season].index=[i for i in range(len(saved_plot_df[season]))]\n",
    "    drop=[]\n",
    "    for i in range(len(saved_plot_df[season])):\n",
    "        if saved_plot_df[season].iloc[i, 0]==0:\n",
    "            drop.append(i)\n",
    "    for index in drop:\n",
    "        saved_plot_df[season] = saved_plot_df[season].drop(index=[index])\n",
    "    saved_plot_df[season].index = [i for i in range(len(saved_plot_df[season]))]\n",
    "\n",
    "    saved_plot_df[season].columns=['Actual Bee Activity', 'Predicted Bee Activity']\n",
    "\n",
    "    # fig, ax=plt.subplots(figsize=(10, 10))\n",
    "    sns.scatterplot(data=saved_plot_df[season], x='Actual Bee Activity', y='Predicted Bee Activity', palette=['skyblue'])\n",
    "    # sns.lineplot(data=pd.DataFrame([[0, 0], [max_val, max_val]]), palette=['skyblue'], legend=False)\n",
    "    sns.lineplot(data=pd.DataFrame([[0, 0], [max_val, max_val]], columns=['x', 'y']), x='x', y='y', legend=False)\n",
    "    plt.title('Actual Bee Activity vs. Predicted Bee Activity ('+seasons[season]+')')\n",
    "    if season==2:\n",
    "        plt.savefig(os.path.join(directory, 'Figures',\n",
    "                    'actual_vs_predicted_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.cla()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    max_val=0\n",
    "    for i in range(len(saved_plot_df[season])):\n",
    "        for j in range(2):\n",
    "            if saved_plot_df[season].iloc[i, j]>max_val:\n",
    "                max_val=saved_plot_df[season].iloc[i, j]\n",
    "\n",
    "    saved_plot_df[season].index=[i for i in range(len(saved_plot_df[season]))]\n",
    "    drop=[]\n",
    "    for i in range(len(saved_plot_df[season])):\n",
    "        if saved_plot_df[season].iloc[i, 0]==0:\n",
    "            drop.append(i)\n",
    "    for index in drop:\n",
    "        saved_plot_df[season] = saved_plot_df[season].drop(index=[index])\n",
    "    saved_plot_df[season].index = [i for i in range(len(saved_plot_df[season]))]\n",
    "\n",
    "    saved_plot_df[season].columns=['Actual Bee Activity', 'Predicted Bee Activity']\n",
    "\n",
    "    # fig, ax=plt.subplots(figsize=(10, 10))\n",
    "    sns.scatterplot(data=saved_plot_df[season], x='Actual Bee Activity', y='Predicted Bee Activity', palette=['skyblue'])\n",
    "    # sns.lineplot(data=pd.DataFrame([[0, 0], [max_val, max_val]]), palette=['skyblue'], legend=False)\n",
    "    sns.lineplot(data=pd.DataFrame([[0, 0], [max_val, max_val]], columns=['x', 'y']), x='x', y='y', legend=False)\n",
    "    plt.title('Actual Bee Activity vs. Predicted Bee Activity ('+seasons[season]+')')\n",
    "    # plt.savefig(os.path.join(directory, 'Figures',\n",
    "    #             'actual_vs_predicted_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.cla()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_arr=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = [pd.DataFrame() for i in range(3)]\n",
    "\n",
    "optimal_features = [[0, 2, 3, 8], [0, 2, 3, 5, 8], [0, 2, 3, 5, 8]]\n",
    "\n",
    "# weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "# weather_columns=[0, 2, 8]\n",
    "model_type = 1\n",
    "# season = 0\n",
    "time_steps = 9\n",
    "\n",
    "for season in range(2,3):\n",
    "    index = 0\n",
    "    X = []\n",
    "\n",
    "    weather_columns = optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    # row = [[], []]\n",
    "    r2_arr.append([])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, activation='relu', return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64, activation='relu',\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "        model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50,\n",
    "                  batch_size=16, verbose=1)\n",
    "\n",
    "        y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "        combined = pd.concat(\n",
    "            [pd.DataFrame(Y_test), pd.DataFrame(y_pred)], axis=1)\n",
    "        plot_df[season] = pd.concat(\n",
    "            [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        # row[1].append(r2)\n",
    "        r2_arr[-1].append(r2)\n",
    "\n",
    "    # print(pd.DataFrame(row[1]).mean())\n",
    "    # row_name = models[model_type]\n",
    "    # for factor in drop_combinations[i_drop]:\n",
    "    #      row_name+='-'+factor[:1]\n",
    "\n",
    "# row_name = models[model_type]\n",
    "# if len(drop_combinations[i_drop]) >= 1:\n",
    "#     row_name += '-'\n",
    "# row_name += '-'.join([factor[:1]\n",
    "#                         for factor in drop_combinations[i_drop]])\n",
    "# df[0][row_name] = row[0]\n",
    "# df[1][row_name] = row[1]\n",
    "# print(row_name)\n",
    "\n",
    "# subset_metrics[0][-1].append(df[0].transpose())\n",
    "# subset_metrics[1][-1].append(df[1].transpose())\n",
    "# r2 = r2_score(Y_test, y_pred)\n",
    "    # model_r2.append(r2)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bee hour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, activation='relu',\n",
    "        input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    LSTM(100, activation='relu'),\n",
    "    Dropout(0.15),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "time_steps=24\n",
    "X = []\n",
    "\n",
    "# time_steps = optimal_time_steps[model_type][season]\n",
    "index = 0\n",
    "\n",
    "for date in data_blocks[season]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X = np.array(X)\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "tests = 10\n",
    "kf = KFold(n_splits=tests)\n",
    "\n",
    "r2_arr.append([])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scalers = []\n",
    "    for i in range(X_train.shape[2]):\n",
    "        scalers.append(StandardScaler())\n",
    "        X_train[:, :, i] = scalers[i].fit_transform(\n",
    "            X_train[:, :, i])\n",
    "    for i in range(X_test.shape[2]):\n",
    "        X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=50,\n",
    "            batch_size=16, validation_data=(X_test, Y_test), verbose=1)\n",
    "    # model.fit(X_train, Y_train, epochs=40,\n",
    "    #         batch_size=128, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "    \n",
    "    # mse = mean_squared_error(Y_test, y_pred)\n",
    "    # row[0].append(mse)\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    # row[1].append(r2)\n",
    "    r2_arr[-1].append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r2_arr[-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 8\n",
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 nodes\n",
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 nodes\n",
    "pd.DataFrame(row[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row[1]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "\n",
    "for season in range(1, 3):\n",
    "    # model = Sequential([\n",
    "    #     GRU(64, return_sequences=True, activation='relu',\n",
    "    #         input_shape=(time_steps, len(weather_columns))),\n",
    "    #     Dropout(0.2),\n",
    "    #     GRU(64, activation='relu'),\n",
    "    #     Dropout(0.2),\n",
    "    #     Dense(1)\n",
    "    # ])\n",
    "    # model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "    time_steps=24\n",
    "    X = []\n",
    "\n",
    "    # time_steps = optimal_time_steps[model_type][season]\n",
    "    index = 0\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    r2_arr.append([])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True, activation='relu',\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "        model.fit(X_train, Y_train, epochs=50,\n",
    "                batch_size=16, validation_data=(X_test, Y_test), verbose=1)\n",
    "        # model.fit(X_train, Y_train, epochs=40,\n",
    "        #         batch_size=128, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "        \n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        # row[1].append(r2)\n",
    "        r2_arr[-1].append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "\n",
    "# model = Sequential([\n",
    "#     GRU(64, return_sequences=True, activation='relu',\n",
    "#         input_shape=(time_steps, len(weather_columns))),\n",
    "#     Dropout(0.2),\n",
    "#     GRU(64, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "season=2\n",
    "\n",
    "time_steps = 24\n",
    "X = []\n",
    "\n",
    "# time_steps = optimal_time_steps[model_type][season]\n",
    "index = 0\n",
    "\n",
    "for date in data_blocks[season]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X = np.array(X)\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "tests = 10\n",
    "kf = KFold(n_splits=tests)\n",
    "\n",
    "r2_arr.append([])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True, activation='relu',\n",
    "            input_shape=(time_steps, len(weather_columns))),\n",
    "        Dropout(0.2),\n",
    "        GRU(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scalers = []\n",
    "    for i in range(X_train.shape[2]):\n",
    "        scalers.append(StandardScaler())\n",
    "        X_train[:, :, i] = scalers[i].fit_transform(\n",
    "            X_train[:, :, i])\n",
    "    for i in range(X_test.shape[2]):\n",
    "        X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=50,\n",
    "            batch_size=16, validation_data=(X_test, Y_test), verbose=1)\n",
    "    # model.fit(X_train, Y_train, epochs=40,\n",
    "    #         batch_size=128, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    # mse = mean_squared_error(Y_test, y_pred)\n",
    "    # row[0].append(mse)\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    # row[1].append(r2)\n",
    "    r2_arr[-1].append(r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bee hour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "# weather_columns=[0, 2, 8]\n",
    "model_type = 1\n",
    "pred_dates=[set() for i in range(3)]\n",
    "binary_pred=[{} for i in range(3)]\n",
    "bee_hour = [{} for i in range(3)]\n",
    "bee_hour2 = [{} for i in range(3)]\n",
    "time_steps=9\n",
    "\n",
    "for season in range(3):\n",
    "    X = []\n",
    "\n",
    "    # time_steps = optimal_time_steps[model_type][season]\n",
    "    index = 0\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     GRU(64, return_sequences=True, activation='relu',\n",
    "    #         input_shape=(time_steps, len(weather_columns))),\n",
    "    #     Dropout(0.2),\n",
    "    #     GRU(64, activation='relu'),\n",
    "    #     Dropout(0.2),\n",
    "    #     Dense(1)\n",
    "    # ])\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    row = [[], []]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        \n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        X_train_scaled = np.asarray(X_train).astype('float32')\n",
    "        X_test_scaled = np.asarray(X_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train_scaled[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train_scaled[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test_scaled[:, :, i] = scalers[i].transform(X_test_scaled[:, :, i])\n",
    "\n",
    "        model.fit(X_train_scaled, Y_train, epochs=10,\n",
    "                batch_size=16, verbose=0)\n",
    "        # model.fit(X_train, Y_train, epochs=40,\n",
    "        #         batch_size=128, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            date=bee_np[season][test_index[i]][-1]\n",
    "            pred_dates[season].add(date)\n",
    "            if not date in binary_pred[season]:\n",
    "                binary_pred[season][date]=0\n",
    "            if not date in bee_hour[season]:\n",
    "                bee_hour[season][date]=0\n",
    "            if not date in bee_hour2[season]:\n",
    "                bee_hour2[season][date]=0\n",
    "            if y_pred[i]>=0.3:\n",
    "                binary_pred[season][date]+=1\n",
    "            if X_test[i][-1][0]>55 and X_test[i][-1][2]<15 and X_test[i][-1][4]==0:\n",
    "                bee_hour[season][date] += 1\n",
    "            if X_test[i][-1][0] > 59 and X_test[i][-1][2] < 10 and X_test[i][-1][4] == 0:\n",
    "                bee_hour2[season][date] += 1\n",
    "\n",
    "        # combined = pd.concat([pd.DataFrame(y_pred), pd.DataFrame(Y_test)], axis=1)\n",
    "        # plot_df[season] = pd.concat(\n",
    "        #     [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        row[1].append(r2)\n",
    "        # if plot_df.iloc[i, 0] == 0:\n",
    "        #     plot_df.drop(index=[i])\n",
    "    print(pd.DataFrame(row[1]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "              style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "for season in range(3):\n",
    "    plt.cla()\n",
    "    indices = []\n",
    "    for i in sorted(pred_dates[season]):\n",
    "        indices.append(i)\n",
    "\n",
    "    indexed = []\n",
    "    indexed2 = []\n",
    "    indexed3 = []\n",
    "    for date in indices:\n",
    "        indexed.append(binary_pred[season][date])\n",
    "        indexed2.append(bee_hour[season][date])\n",
    "        indexed3.append(bee_hour2[season][date])\n",
    "\n",
    "    indexed = pd.DataFrame(indexed)\n",
    "    indexed.index = indices\n",
    "    indexed.columns = ['Model Prediction']\n",
    "    indexed2 = pd.DataFrame(indexed2)\n",
    "    indexed2.index = indices\n",
    "    indexed2.columns = ['Bee Hour Calculation (Version 1)']\n",
    "    indexed3= pd.DataFrame(indexed3)\n",
    "    indexed3.index = indices\n",
    "    indexed3.columns = ['Bee Hour Calculation (Version 2)']\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    # ax.xaxis.set_ticks(indices)\n",
    "    # ax.xaxis.set_ticklabels(indices)\n",
    "    # ax.xaxis.set_ticks_position('bottom')\n",
    "    # ax.yaxis.set_ticks_position('left')\n",
    "    # plt.xticks(rotation=0)\n",
    "\n",
    "    sns.lineplot(data=indexed, palette=['green'])\n",
    "    sns.lineplot(data=indexed2, palette=['red'])\n",
    "    sns.lineplot(data=indexed3, palette=['blue'])\n",
    "\n",
    "    # indexed.plot(ax=ax, legend=True)\n",
    "    # indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Bee Hours')\n",
    "    plt.title(\n",
    "        'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "    # plt.savefig(os.path.join(directory, 'Figures',\n",
    "    #             'bee_hour_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full bee hour plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.iloc[0, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[train_index][:, :, [weather_columns]].squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H5\n",
    "\n",
    "# weather_columns_full = [[0, 2, 3, 6, 8], [0, 2, 3, 5, 8], [0, 2, 3, 5, 8]]\n",
    "optimal_features=[[0, 2, 3, 5, 6, 8], [0, 2, 3, 8], [0, 2, 3, 6, 8]]\n",
    "# weather_columns=[0, 2, 8]\n",
    "pred_dates=[set() for i in range(3)]\n",
    "scaled_pred=[{} for i in range(3)]\n",
    "scaled_actual=[{} for i in range(3)]\n",
    "binary_pred=[{} for i in range(3)]\n",
    "bee_hour = [{} for i in range(3)]\n",
    "binary_actual=[{} for i in range(3)]\n",
    "# bee_hour2 = [{} for i in range(3)]\n",
    "time_steps=9\n",
    "\n",
    "for season in range(3):\n",
    "    X = []\n",
    "\n",
    "    # time_steps = optimal_time_steps[model_type][season]\n",
    "    index = 0\n",
    "    weather_columns=optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    # print(j, date, j-i-time_steps)\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, :-1].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     GRU(64, return_sequences=True, activation='relu',\n",
    "    #         input_shape=(time_steps, len(weather_columns))),\n",
    "    #     Dropout(0.2),\n",
    "    #     GRU(64, activation='relu'),\n",
    "    #     Dropout(0.2),\n",
    "    #     Dense(1)\n",
    "    # ])\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    row = [[], []]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        \n",
    "        X_train = X[train_index][:, :, [weather_columns]].squeeze()\n",
    "        X_test = X[test_index][:, :, [weather_columns]].squeeze()\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        X_train_scaled = np.asarray(X_train).astype('float32')\n",
    "        X_test_scaled = np.asarray(X_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train_scaled[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train_scaled[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test_scaled[:, :, i] = scalers[i].transform(X_test_scaled[:, :, i])\n",
    "\n",
    "        model.fit(X_train_scaled, Y_train, epochs=50,\n",
    "                batch_size=16, verbose=0)\n",
    "        # model.fit(X_train, Y_train, epochs=40,\n",
    "        #         batch_size=128, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            date=bee_np[season][test_index[i]][-1]\n",
    "            pred_dates[season].add(date)\n",
    "            if not date in scaled_pred[season]:\n",
    "                scaled_pred[season][date] = 0\n",
    "            if not date in scaled_actual[season]:\n",
    "                scaled_actual[season][date] = 0\n",
    "            if not date in bee_hour[season]:\n",
    "                bee_hour[season][date] = 0\n",
    "            if not date in binary_pred[season]:\n",
    "                binary_pred[season][date] = 0\n",
    "\n",
    "            if y_pred[i] >= 0.3:\n",
    "                binary_pred[season][date] += 1\n",
    "            if Y_test[i] >= 0.3:\n",
    "                binary_actual[season][date] += 1\n",
    "            scaled_pred[season][date]+=y_pred[i]/0.3\n",
    "            scaled_actual[season][date]+=Y_test[i]/0.3\n",
    "            if X[test_index][i][-1][0]>55 and X[test_index][i][-1][3]<15 and X[test_index][i][-1][6]==0:\n",
    "                bee_hour[season][date] += 1\n",
    "\n",
    "        # combined = pd.concat([pd.DataFrame(y_pred), pd.DataFrame(Y_test)], axis=1)\n",
    "        # plot_df[season] = pd.concat(\n",
    "        #     [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        row[1].append(r2)\n",
    "        # if plot_df.iloc[i, 0] == 0:\n",
    "        #     plot_df.drop(index=[i])\n",
    "    print(pd.DataFrame(row[1]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H5\n",
    "\n",
    "# weather_columns_full = [[0, 2, 3, 6, 8], [0, 2, 3, 5, 8], [0, 2, 3, 5, 8]]\n",
    "optimal_features=[[0, 2, 3, 5, 6, 8], [0, 2, 3, 8], [0, 2, 3, 6, 8]]\n",
    "# weather_columns=[0, 2, 8]\n",
    "pred_dates=[set() for i in range(3)]\n",
    "scaled_pred=[{} for i in range(3)]\n",
    "scaled_actual=[{} for i in range(3)]\n",
    "binary_pred=[{} for i in range(3)]\n",
    "bee_hour = [{} for i in range(3)]\n",
    "binary_actual=[{} for i in range(3)]\n",
    "# bee_hour2 = [{} for i in range(3)]\n",
    "time_steps=9\n",
    "\n",
    "for season in range(3):\n",
    "    X = []\n",
    "\n",
    "    # time_steps = optimal_time_steps[model_type][season]\n",
    "    index = 0\n",
    "    weather_columns=optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    # print(j, date, j-i-time_steps)\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, :-1].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # drop_combinations = []\n",
    "\n",
    "    # for i in range(len(leave_out[season])+1):\n",
    "    #     for j in itertools.combinations(leave_out[season], i):\n",
    "    #         temp = []\n",
    "    #         for k in j:\n",
    "    #             temp.append(k)\n",
    "    #         drop_combinations.append(temp)\n",
    "\n",
    "    # printdrop_combinations)\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     GRU(64, return_sequences=True, activation='relu',\n",
    "    #         input_shape=(time_steps, len(weather_columns))),\n",
    "    #     Dropout(0.2),\n",
    "    #     GRU(64, activation='relu'),\n",
    "    #     Dropout(0.2),\n",
    "    #     Dense(1)\n",
    "    # ])\n",
    "\n",
    "    tests = 10\n",
    "    kf = KFold(n_splits=tests)\n",
    "\n",
    "    row = [[], []]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True,\n",
    "                input_shape=(time_steps, len(weather_columns))),\n",
    "            Dropout(0.2),\n",
    "            GRU(64),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        # (learning_rate=0.01)\n",
    "        model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "        \n",
    "        X_train = X[train_index][:, :, [weather_columns]].squeeze()\n",
    "        X_test = X[test_index][:, :, [weather_columns]].squeeze()\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        X_train_scaled = np.asarray(X_train).astype('float32')\n",
    "        X_test_scaled = np.asarray(X_test).astype('float32')\n",
    "\n",
    "        scalers = []\n",
    "        for i in range(X_train.shape[2]):\n",
    "            scalers.append(StandardScaler())\n",
    "            X_train_scaled[:, :, i] = scalers[i].fit_transform(\n",
    "                X_train_scaled[:, :, i])\n",
    "        for i in range(X_test.shape[2]):\n",
    "            X_test_scaled[:, :, i] = scalers[i].transform(X_test_scaled[:, :, i])\n",
    "\n",
    "        model.fit(X_train_scaled, Y_train, epochs=50,\n",
    "                batch_size=16, verbose=0)\n",
    "        # model.fit(X_train, Y_train, epochs=40,\n",
    "        #         batch_size=128, verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            date=bee_np[season][test_index[i]][-1]\n",
    "            pred_dates[season].add(date)\n",
    "            if not date in scaled_pred[season]:\n",
    "                scaled_pred[season][date] = 0\n",
    "            if not date in scaled_actual[season]:\n",
    "                scaled_actual[season][date] = 0\n",
    "            if not date in bee_hour[season]:\n",
    "                bee_hour[season][date] = 0\n",
    "            if not date in binary_pred[season]:\n",
    "                binary_pred[season][date] = 0\n",
    "            if not date in binary_actual[season]:\n",
    "                binary_actual[season][date] = 0\n",
    "\n",
    "            if y_pred[i] >= 0.3:\n",
    "                binary_pred[season][date] += 1\n",
    "            if Y_test[i] >= 0.3:\n",
    "                binary_actual[season][date] += 1\n",
    "            scaled_pred[season][date]+=y_pred[i]/0.3\n",
    "            scaled_actual[season][date]+=Y_test[i]/0.3\n",
    "            if X[test_index][i][-1][0]>55 and X[test_index][i][-1][3]<15 and X[test_index][i][-1][6]==0:\n",
    "                bee_hour[season][date] += 1\n",
    "\n",
    "        # combined = pd.concat([pd.DataFrame(y_pred), pd.DataFrame(Y_test)], axis=1)\n",
    "        # plot_df[season] = pd.concat(\n",
    "        #     [plot_df[season], combined], axis=0, ignore_index=True)\n",
    "        # mse = mean_squared_error(Y_test, y_pred)\n",
    "        # row[0].append(mse)\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        row[1].append(r2)\n",
    "        # if plot_df.iloc[i, 0] == 0:\n",
    "        #     plot_df.drop(index=[i])\n",
    "    print(pd.DataFrame(row[1]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.84\n",
    "exclude both:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude pressure: 0.802082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude rainfall: 0.811197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.839882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1] .823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons=[\"Summer\", \"Fall\", \"Spring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "              style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "num_lines = 3\n",
    "line_values = [scaled_pred, scaled_actual, bee_hour]\n",
    "line_labels = ['Model Prediction',\n",
    "               'Actual Bee Activity', 'Bee Hour Calculation']\n",
    "line_colors = ['green', 'blue', 'red']\n",
    "\n",
    "for season in range(3):\n",
    "    plt.cla()\n",
    "    indices = []\n",
    "    for i in sorted(pred_dates[season]):\n",
    "        indices.append(i)\n",
    "\n",
    "    indexed = [[] for i in range(num_lines)]\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        for date in indices:\n",
    "            indexed[i].append(line_values[i][season][date])\n",
    "\n",
    "        indexed[i] = pd.DataFrame(indexed[i])\n",
    "        indexed[i].index = indices\n",
    "        indexed[i].columns = [line_labels[i]]\n",
    "        sns.lineplot(data=indexed[i], palette=[line_colors[i]])\n",
    "\n",
    "    # indexed3.columns = ['Model Prediction (Binary)']\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    # ax.xaxis.set_ticks(indices)\n",
    "    # ax.xaxis.set_ticklabels(indices)\n",
    "    # ax.xaxis.set_ticks_position('bottom')\n",
    "    # ax.yaxis.set_ticks_position('left')\n",
    "    # plt.xticks(rotation=0)\n",
    "\n",
    "    # sns.lineplot(data=indexed, palette=['green'])\n",
    "    # sns.lineplot(data=indexed2, palette=['red'])\n",
    "    # sns.lineplot(data=indexed3, palette=['blue'])\n",
    "    # sns.lineplot(data=indexed4, palette=['orange'])\n",
    "\n",
    "    # indexed.plot(ax=ax, legend=True)\n",
    "    # indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Bee Hours')\n",
    "    plt.title(\n",
    "        'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "    # plt.savefig(os.path.join(directory, 'Figures',\n",
    "    #             'bee_hour_scaled_comp1_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "                    style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "num_lines=3\n",
    "line_values=[scaled_pred, scaled_actual, bee_hour]\n",
    "line_labels=['Model Prediction', 'Actual Bee Activity', 'Bee Hour Calculation']\n",
    "line_colors=['green', 'blue', 'red']\n",
    "\n",
    "for season in range(3):\n",
    "    plt.cla()\n",
    "    indices = []\n",
    "    for i in sorted(pred_dates[season]):\n",
    "        indices.append(i)\n",
    "\n",
    "    indexed=[[] for i in range(num_lines)]\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        for date in indices:\n",
    "            indexed[i].append(line_values[i][season][date])\n",
    "\n",
    "        indexed[i] = pd.DataFrame(indexed[i])\n",
    "        indexed[i].index = indices\n",
    "        indexed[i].columns = [line_labels[i]]\n",
    "        sns.lineplot(data=indexed[i], palette=[line_colors[i]])\n",
    "\n",
    "    # indexed3.columns = ['Model Prediction (Binary)']\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    # ax.xaxis.set_ticks(indices)\n",
    "    # ax.xaxis.set_ticklabels(indices)\n",
    "    # ax.xaxis.set_ticks_position('bottom')\n",
    "    # ax.yaxis.set_ticks_position('left')\n",
    "    # plt.xticks(rotation=0)\n",
    "\n",
    "    # sns.lineplot(data=indexed, palette=['green'])\n",
    "    # sns.lineplot(data=indexed2, palette=['red'])\n",
    "    # sns.lineplot(data=indexed3, palette=['blue'])\n",
    "    # sns.lineplot(data=indexed4, palette=['orange'])\n",
    "\n",
    "    # indexed.plot(ax=ax, legend=True)\n",
    "    # indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Bee Hours')\n",
    "    plt.title(\n",
    "        'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "    # plt.savefig(os.path.join(directory, 'Figures',\n",
    "    #             'bee_hour_scaled_comp1_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "                    style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "line_values = [binary_pred, bee_hour, binary_actual]\n",
    "line_labels=['Model Prediction (Binary)', 'Bee Hour Calculation', 'Actual Bee Activity (Binary)']\n",
    "line_colors=['green', 'blue', 'red']\n",
    "num_lines=len(line_values)\n",
    "\n",
    "for season in range(3):\n",
    "    plt.cla()\n",
    "    indices = []\n",
    "    for i in sorted(pred_dates[season]):\n",
    "        indices.append(i)\n",
    "\n",
    "    indexed=[[] for i in range(num_lines)]\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        for date in indices:\n",
    "            indexed[i].append(line_values[i][season][date])\n",
    "\n",
    "        indexed[i] = pd.DataFrame(indexed[i])\n",
    "        indexed[i].index = indices\n",
    "        indexed[i].columns = [line_labels[i]]\n",
    "        sns.lineplot(data=indexed[i], palette=[line_colors[i]])\n",
    "\n",
    "    # indexed3.columns = ['Model Prediction (Binary)']\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    # ax.xaxis.set_ticks(indices)\n",
    "    # ax.xaxis.set_ticklabels(indices)\n",
    "    # ax.xaxis.set_ticks_position('bottom')\n",
    "    # ax.yaxis.set_ticks_position('left')\n",
    "    # plt.xticks(rotation=0)\n",
    "\n",
    "    # sns.lineplot(data=indexed, palette=['green'])\n",
    "    # sns.lineplot(data=indexed2, palette=['red'])\n",
    "    # sns.lineplot(data=indexed3, palette=['blue'])\n",
    "    # sns.lineplot(data=indexed4, palette=['orange'])\n",
    "\n",
    "    # indexed.plot(ax=ax, legend=True)\n",
    "    # indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Bee Hours')\n",
    "    plt.title(\n",
    "        'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "    # plt.savefig(os.path.join(directory, 'Figures',\n",
    "    #             'bee_hour_scaled_comp1_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(pred_dates[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in range(3):\n",
    "    temp=pd.concat([pd.DataFrame(list(scaled_pred[season].values())), \n",
    "                    pd.DataFrame(list(scaled_actual[season].values())),\n",
    "                                 pd.DataFrame(list(binary_pred[season].values())),\n",
    "    pd.DataFrame(list(bee_hour[season].values())), \n",
    "    pd.DataFrame(list(binary_actual[season].values()))], axis=1)\n",
    "    temp.columns=['Model Prediction', 'Actual Bee Activity',\n",
    "                'Model Prediction (Binary)', 'Bee Hour Calculation', 'Actual Bee Activity (Binary)']\n",
    "    temp.index=sorted(list(pred_dates[season]))\n",
    "    temp.to_csv(os.path.join(directory, processed_folder, \"prediction_\"+seasons[season]+\".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv(\"/Users/vincewu/Bees-Project/binary_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_pred[2][pd.to_datetime(\"2023-02-15\")], scaled_actual[2][pd.to_datetime(\"2023-02-15\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_pred[2][pd.to_datetime(\"2023-01-30\")],\n",
    "      scaled_actual[2][pd.to_datetime(\"2023-01-30\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pred[2][pd.to_datetime(\"2023-02-25\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_actual[2][pd.to_datetime(\"2023-02-25\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "                    style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "for season in range(3):\n",
    "    plt.cla()\n",
    "    indices = []\n",
    "    for i in sorted(pred_dates[season]):\n",
    "        indices.append(i)\n",
    "\n",
    "    indexed = []\n",
    "    indexed2 = []\n",
    "    indexed3 = []\n",
    "    for date in indices:\n",
    "        indexed.append(scaled_pred[season][date])\n",
    "        indexed2.append(bee_hour[season][date])\n",
    "        indexed3.append(scaled_actual[season][date])\n",
    "\n",
    "    indexed = pd.DataFrame(indexed)\n",
    "    indexed.index = indices\n",
    "    indexed.columns = ['Model Prediction (Scaled)']\n",
    "    indexed2 = pd.DataFrame(indexed2)\n",
    "    indexed2.index = indices\n",
    "    indexed2.columns = ['Bee Hour Calculation (Binary)']\n",
    "    indexed3 = pd.DataFrame(indexed3)\n",
    "    indexed3.index = indices\n",
    "    indexed3.columns = ['Actual Activity (Scaled)']\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # sns.lineplot(data=indexed, ax=ax)\n",
    "    # sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "    # ax.set_ylim(bottom=0, top=0.25)\n",
    "    # ax.xaxis.set_ticks(indices)\n",
    "    # ax.xaxis.set_ticklabels(indices)\n",
    "    # ax.xaxis.set_ticks_position('bottom')\n",
    "    # ax.yaxis.set_ticks_position('left')\n",
    "    # plt.xticks(rotation=0)\n",
    "\n",
    "    sns.lineplot(data=indexed, palette=['green'])\n",
    "    sns.lineplot(data=indexed2, palette=['red'])\n",
    "    sns.lineplot(data=indexed3, palette=['blue'])\n",
    "\n",
    "    # indexed.plot(ax=ax, legend=True)\n",
    "    # indexed2.plot(ax=ax, legend=True, c='r')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Daily Bee Hours')\n",
    "    plt.title(\n",
    "        'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "    plt.savefig(os.path.join(directory, 'Figures',\n",
    "                'bee_hour_scaled_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([indexed, indexed2,indexed3], axis=1).to_csv(os.path.join(directory, '1Test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_np[0][:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saved seasonal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features=[[0, 2, 3, 5, 6, 8], [0, 2, 3, 8], [0, 2, 3, 6, 8]]\n",
    "\n",
    "time_steps = 9\n",
    "\n",
    "for season in range(3):\n",
    "    index = 0\n",
    "    X = []\n",
    "\n",
    "    weather_columns=optimal_features[season]\n",
    "\n",
    "    for date in data_blocks[season]:\n",
    "        for i in range(index, len(weather_data)):\n",
    "            if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "                temp = []\n",
    "                # print('name', weather_data.iloc[i].name)\n",
    "                for j in range(i, i+time_steps+24):\n",
    "                    temp.append(\n",
    "                        weather_data.iloc[j, weather_columns].values)\n",
    "                for hour in range(24):\n",
    "                    X.append(np.array(temp[hour:hour+time_steps]))\n",
    "                index = i\n",
    "                break\n",
    "    X = np.array(X)\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True,\n",
    "            input_shape=(time_steps, len(weather_columns))),\n",
    "        Dropout(0.2),\n",
    "        GRU(64),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # (learning_rate=0.01)\n",
    "    model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "\n",
    "    # scalers = []\n",
    "    # for i in range(X_train.shape[2]):\n",
    "    #     scalers.append(StandardScaler())\n",
    "    #     X_train[:, :, i] = scalers[i].fit_transform(\n",
    "    #         X_train[:, :, i])\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=50,\n",
    "            batch_size=16, verbose=1)\n",
    "    model.save(os.path.join(directory, \"Final Models\", \"model_\"+seasons[season]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one day hourly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blocks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#days 3/11 and after excluded from graph, so test all\n",
    "\n",
    "season=2\n",
    "temp_data_block = [[],[]]\n",
    "index=0\n",
    "for date in data_blocks[season]:\n",
    "    if date==pd.to_datetime(\"2023-03-11\"):\n",
    "        index+=1\n",
    "    temp_data_block[index].append(date)\n",
    "\n",
    "# weather_columns_full = [[0, 2, 3, 6, 8], [0, 2, 3, 5, 8], [0, 2, 3, 5, 8]]\n",
    "optimal_features=[[0, 2, 3, 5, 6, 8], [0, 2, 3, 8], [0, 2, 3, 6, 8]]\n",
    "# bee_hour2 = [{} for i in range(3)]\n",
    "time_steps=9\n",
    "\n",
    "X_train = []\n",
    "\n",
    "index = 0\n",
    "weather_columns=optimal_features[season]\n",
    "\n",
    "for date in temp_data_block[0]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                # print(j, date, j-i-time_steps)\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, :-1].values)\n",
    "            for hour in range(24):\n",
    "                X_train.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X_train = np.array(X_train)\n",
    "Y_train = pd.DataFrame(bee_np[season][:, 1]).iloc[:X_train.shape[0]]\n",
    "\n",
    "X_test = []\n",
    "for date in temp_data_block[1]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                # print(j, date, j-i-time_steps)\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, :-1].values)\n",
    "            for hour in range(24):\n",
    "                X_test.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X_test = np.array(X_test)\n",
    "Y_test = pd.DataFrame(bee_np[season][:, 1]).iloc[X_train.shape[0]:]\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, return_sequences=True,\n",
    "        input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.2),\n",
    "    GRU(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "# (learning_rate=0.01)\n",
    "model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "X_train = X_train[:, :, [weather_columns]].squeeze()\n",
    "X_test = X_test[:, :, [weather_columns]].squeeze()\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "X_train_scaled = np.asarray(X_train).astype('float32')\n",
    "X_test_scaled = np.asarray(X_test).astype('float32')\n",
    "\n",
    "scalers = []\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers.append(StandardScaler())\n",
    "    X_train_scaled[:, :, i] = scalers[i].fit_transform(\n",
    "        X_train_scaled[:, :, i])\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test_scaled[:, :, i] = scalers[i].transform(X_test_scaled[:, :, i])\n",
    "\n",
    "model.fit(X_train_scaled, Y_train, epochs=50,\n",
    "        batch_size=16, verbose=0)\n",
    "# model.fit(X_train, Y_train, epochs=40,\n",
    "#         batch_size=128, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled).reshape(-1, 1)\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred[i, 0]=max(y_pred[i, 0], 0)\n",
    "\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(y_pred)-1, 24):\n",
    "    print(r2_score(Y_test[i: i+24], y_pred[i: i+24]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(y_pred)-24, 24):\n",
    "    print(r2_score(Y_test[i: i+24], y_pred[i: i+24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r2_score(Y_test[2*24: 3*24], y_pred[2*24: 3*24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r2_score(Y_test[4*24: 5*24], y_pred[4*24: 5*24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_block[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (15, 6)},\n",
    "              style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "date_str=\"3/25/23\"\n",
    "\n",
    "num_lines = 3\n",
    "line_values = [Y_test[3*24: 6*24], y_pred[3*24: 6*24]]\n",
    "line_labels = ['Measured Bee Activity', 'Model Prediction']\n",
    "line_colors = ['green', 'blue']\n",
    "\n",
    "for i in range(2):\n",
    "    line_values[i]=pd.DataFrame(line_values[i])\n",
    "    line_values[i].columns = [line_labels[i]]\n",
    "    sns.lineplot(data=line_values[i], palette=[line_colors[i]])\n",
    "plt.xlabel('Hour')\n",
    "plt.xticks(range(0, 73, 4))\n",
    "plt.ylabel('Percentage of Foragers Outside')\n",
    "plt.title(\n",
    "    'Measured Bee Activity vs. Model Prediction (3/24/23-3/26/23)')\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'hourly_predictions.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.concat([pd.DataFrame(Y_test[3*24: 6*24]), pd.DataFrame(y_pred[3*24: 6*24])], axis=1)\n",
    "temp.columns=['Measured Bee Activity', 'Model Prediction']\n",
    "temp.to_csv(os.path.join(directory, processed_folder,\n",
    "            'hourly_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(rc={'figure.figsize': (10, 6)},\n",
    "                    style='white', font='Arial', font_scale=1.1)\n",
    "\n",
    "num_lines=3\n",
    "line_values=[Y_test[2*24: 3*24], y_pred[2*24: 3*24]]\n",
    "line_labels=['Model Prediction', 'Actual Bee Activity']\n",
    "line_colors=['green', 'blue']\n",
    "\n",
    "for i in range(2):\n",
    "    sns.lineplot(data=line_values[i], palette=[line_colors[i]])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Bee Hours')\n",
    "plt.title(\n",
    "    'Daily Bee Hours: Model Prediction vs. Bee Hour Calculation (' + seasons[season]+')')\n",
    "# plt.savefig(os.path.join(directory, 'Figures',\n",
    "#             'bee_hour_scaled_comp1_'+seasons[season]+'.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6am-6pm\n",
    "march_7=[0, 32, 247, 435, 643, 812, 854, 883, 873, 787, 588, 313, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_num = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "weather_headers = ['Temp.', 'Dew Point', 'Humidity', 'Speed',\n",
    "                   'Gust', 'Pressure', 'Rainfall', 'UV', 'Solar']\n",
    "\n",
    "weather_file = 'weather_forecast_clean.csv'\n",
    "weather_forecast = pd.read_csv(os.path.join(directory, processed_folder, weather_file),\n",
    "                           parse_dates=['Datetime'], engine='python')\n",
    "weather_forecast = weather_forecast.resample('1h', on='Datetime').mean()\n",
    "# weather_forecast.columns = weather_headers\n",
    "weather_forecast.insert(len(weather_forecast.columns),\n",
    "                    'Datetime_col', weather_forecast.index.tolist())\n",
    "weather_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast[7463:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "# weather_columns=[0, 2, 8]\n",
    "X = []\n",
    "model_type = 1\n",
    "season = 2\n",
    "time_steps = 9\n",
    "index = 0\n",
    "\n",
    "for date in data_blocks[season]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name == date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp = []\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(\n",
    "                    weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index = i\n",
    "            break\n",
    "X = np.array(X)\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# drop_combinations = []\n",
    "\n",
    "# for i in range(len(leave_out[season])+1):\n",
    "#     for j in itertools.combinations(leave_out[season], i):\n",
    "#         temp = []\n",
    "#         for k in j:\n",
    "#             temp.append(k)\n",
    "#         drop_combinations.append(temp)\n",
    "\n",
    "# printdrop_combinations)\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, activation='relu', return_sequences=True,\n",
    "        input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.2),\n",
    "    GRU(64, activation='relu',\n",
    "        input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# (learning_rate=0.01)\n",
    "model.compile(metrics=['mse'], loss='mse', optimizer='adam')\n",
    "\n",
    "X_train = X\n",
    "Y_train = Y\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "\n",
    "scalers = []\n",
    "for i in range(X_train.shape[2]):\n",
    "    scalers.append(StandardScaler())\n",
    "    X_train[:, :, i] = scalers[i].fit_transform(\n",
    "        X_train[:, :, i])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50,\n",
    "            batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([weather_forecast.iloc[7455:, [\n",
    "                       0, 2, 3, 5, 6, 8]].to_numpy()]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array([weather_forecast.iloc[7455-(23-i):7455-(23-i)+9, [\n",
    "    0, 2, 3, 5, 6, 8]].to_numpy() for i in range(24)])\n",
    "for i in range(X_test.shape[2]):\n",
    "    X_test[:, :, i] = scalers[i].transform(X_test[:, :, i])\n",
    "\n",
    "y_pred = model.predict(X_test).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_y_pred=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(8, 8))\n",
    "# sns.lineplot(data=pd.DataFrame(forecast_y_pred))\n",
    "pd.DataFrame(forecast_y_pred).plot(ax=ax, legend=False)\n",
    "plt.xticks([i for i in range(0, 24, 2)])\n",
    "# ax.set_xticks([i for i in range(0, 24, 2)])\n",
    "# ax.set_xticklabels([i for i in range(0, 24, 2)])\n",
    "# ax.spines['left'].set_visible(True)\n",
    "# ax.xaxspines['bottom'].set_visible(True)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Percent of Bees Outside')\n",
    "plt.title('Hourly Model Predictions for 03/07/2023')\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'forecasted_activity.jpg'), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df[5:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(holdon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[holdon]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season=2\n",
    "subset_metrics.append([])\n",
    "#k-fold cross validation\n",
    "# neither precip nor barometric\n",
    "full_weather_columns = [[0, 2, 3, 5, 6, 8], [\n",
    "    0, 2, 3, 5, 8], [0, 2, 3, 6, 8], [0, 2, 3, 8]]\n",
    "column_headers = [['LSTM-6', 'GRU-6'], ['LSTM-4+P',\n",
    "                                        'GRU-4+P'], ['LSTM-4+R', 'GRU-4+R'], ['LSTM-4', 'GRU-4']]\n",
    "\n",
    "for i in range(len(full_weather_columns)):\n",
    "    weather_columns = full_weather_columns[i]\n",
    "    df = []\n",
    "    model_gru = Sequential([\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.15),\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(16, activation='relu', return_sequences=True,\n",
    "                input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.15),\n",
    "        LSTM(16, activation='relu', return_sequences=True,\n",
    "                input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # model_mse = []\n",
    "    # model_r2 = []\n",
    "    kf = KFold(n_splits=tests)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "        # print(train_index)\n",
    "        # print(test_index)\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "        X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "        row = []\n",
    "\n",
    "        model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                        batch_size=16, verbose=0)\n",
    "        model_lstm.fit(X_train_scale, Y_train, epochs=3,\n",
    "                        batch_size=16, verbose=0)\n",
    "\n",
    "        y_pred = model_lstm.predict(X_test_scale).reshape(-1, 1)\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        # model_mse.append(mse)\n",
    "        row.append(mse)\n",
    "        y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        # model_mse.append(mse)\n",
    "        row.append(mse)\n",
    "\n",
    "        df.append(row)\n",
    "\n",
    "    subset_metrics[-1].append(pd.DataFrame(df, columns=column_headers[i]))\n",
    "    # r2 = r2_score(Y_test, y_pred)\n",
    "    # model_r2.append(r2)\n",
    "\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final figure generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bee_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "#k-fold cross validation\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "\n",
    "for i in range(len(full_weather_columns)):\n",
    "    weather_columns = full_weather_columns[i]\n",
    "    df = []\n",
    "    model_gru = Sequential([\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.15),\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    model_lstm = Sequential([\n",
    "        LSTM(16, activation='relu', return_sequences=True,\n",
    "             input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.15),\n",
    "        LSTM(16, activation='relu', return_sequences=True,\n",
    "             input_shape=(1, len(weather_columns))),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    # model_mse = []\n",
    "    # model_r2 = []\n",
    "    kf = KFold(n_splits=tests)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "        # print(train_index)\n",
    "        # print(test_index)\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "        X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "        row = []\n",
    "\n",
    "        model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                      batch_size=16, verbose=0)\n",
    "        model_lstm.fit(X_train_scale, Y_train, epochs=3,\n",
    "                       batch_size=16, verbose=0)\n",
    "\n",
    "        y_pred = model_lstm.predict(X_test_scale).reshape(-1, 1)\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        # model_mse.append(mse)\n",
    "        row.append(mse)\n",
    "        y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        # model_mse.append(mse)\n",
    "        row.append(mse)\n",
    "\n",
    "        df.append(row)\n",
    "\n",
    "    subset_metrics[-1].append(pd.DataFrame(df, columns=column_headers[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bee hour predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.concat(\n",
    "    subset_metrics[0], axis=1), color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.title('Model Accuracies on Feature Subsets (Summer)')\n",
    "plt.ylabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'features_summer.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.concat(\n",
    "    subset_metrics[1], axis=1), color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.title('Model Accuracies on Feature Subsets (Fall)')\n",
    "plt.ylabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'features_fall.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.concat(\n",
    "    subset_metrics[-2], axis=1), color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "plt.title('Model Accuracies on Feature Subsets (Early Spring)')\n",
    "plt.ylabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'features_spring_no_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.concat(\n",
    "    subset_metrics[-1], axis=1), color='yellow', medianprops={\"color\": \"black\"}, showfliers=False)\n",
    "plt.title('Model Accuracies on Feature Subsets (Early Spring)')\n",
    "plt.ylabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'features_spring_with_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(subset_metrics[-2], axis=1)\n",
    "for i in df.columns:\n",
    "    print(df[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(subset_metrics[-1], axis=1)\n",
    "for i in df.columns:\n",
    "    print(df[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.concat(\n",
    "    subset_metrics[2], axis=1), color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.title('Model Accuracies on Feature Subsets (Early Spring)')\n",
    "plt.ylabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'features_spring_with_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig\n",
    "sns.boxplot(data=boxplot_df, color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season=0\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "Y_train=Y.astype('float32')\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X).astype('float32')\n",
    "# X_train_scale = X_train\n",
    "# X_test_scale = X_test\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "X_train_single = X_scale.reshape(-1, 1, 6)\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "n_epochs = 25\n",
    "history = model_lstm.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_np[3][10][6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(str(weather_np[3][0][-1])[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "season=3\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_gbr = GradientBoostingRegressor()\n",
    "\n",
    "model_gbr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_gbr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_gbr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_gbr)])\n",
    "cvs=cross_val_score(pipe, X, Y, cv=10)\n",
    "print('R2 cross-validated: ', cvs, cvs.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "season=0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np[season], columns=['Minutes', 'Bees', 'Egress', 'Ingress', 'Both', 'Datetime'])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "\n",
    "for date in data_blocks[2]:\n",
    "    cnt+=arr[date]['Trips Taken']\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "season=0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, 8])\n",
    "# Y = pd.DataFrame(bee_np[season], columns=['Minutes', 'Bees', 'Egress', 'Ingress', 'Both', 'Datetime'])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "season = 1\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np[season], columns=['Minutes', 'Bees', 'Egress', 'Ingress', 'Both', 'Datetime'])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "season=2\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, 3])\n",
    "# Y = pd.DataFrame(bee_np[season], columns=['Minutes', 'Bees', 'Egress', 'Ingress', 'Both', 'Datetime'])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "season=3\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np[season], columns=['Minutes', 'Bees', 'Egress', 'Ingress', 'Both', 'Datetime'])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "\n",
    "model_mse=[]\n",
    "model_r2=[]\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    model_rfr.fit(X_train_scale, Y_train)\n",
    "    y_pred = model_rfr.predict(X_test_scale)\n",
    "    \n",
    "    mse=mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "rfr_mse_1=pd.DataFrame(model_mse)\n",
    "rfr_r2_2=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_r2_2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_r2_2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_r2_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_r2_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_r2_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "season=2\n",
    "\n",
    "# for i in range(0, len(bee_np[season])-initial_hours-24, 24):\n",
    "for i in range(0, len(bee_np[season])-24, 24):\n",
    "    # X_train = X[:initial_hours+i]\n",
    "    # Y_train = Y[:initial_hours+i]\n",
    "    # X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "    # Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "    X_train = X.drop(index=[x for x in range(i, i+24)])\n",
    "    Y_train = Y.drop(index=[x for x in range(i, i+24)])\n",
    "    X_test = X[i:i+24]\n",
    "    Y_test = Y[i:i+24]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_single = scaler.fit_transform(X_train)\n",
    "    X_test_single = scaler.transform(X_test)\n",
    "\n",
    "    model_rfr = RandomForestRegressor()\n",
    "\n",
    "    model_rfr.fit(X_train_single, Y_train)\n",
    "\n",
    "    y_pred = model_rfr.predict(X_test_single)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    # model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "    model_mse.append([weather_np[season][:, -1][i], mse])\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    # model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "    model_r2.append([weather_np[season][:, -1][i], r2])\n",
    "\n",
    "model_r2=pd.DataFrame(model_r2)\n",
    "model_mse=pd.DataFrame(model_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_r2.iloc[:, 1].mean())\n",
    "for i in range(len(model_r2.iloc[:, 1])):\n",
    "    print(i, model_r2.iloc[i, 0], model_r2.iloc[i, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1], columns=['T'])\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "X_train = X[int(len(X)*0.8):]\n",
    "X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "Y_train = Y[int(len(Y)*0.8):]\n",
    "Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 3\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "X_train = X[int(len(X)*0.8):]\n",
    "X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "Y_train = Y[int(len(Y)*0.8):]\n",
    "Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "# X_train_scale = X_train\n",
    "# X_test_scale = X_test\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "X_test_single = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "# only predict 1 variable\n",
    "model_lstm = Sequential([\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_lstm.summary()\n",
    "\n",
    "n_epochs = 25\n",
    "history = model_lstm.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "# X_train = X[int(len(X)*0.8):]\n",
    "# X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "# Y_train = Y[int(len(Y)*0.8):]\n",
    "# Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "# X_train_scale = X_train\n",
    "# X_test_scale = X_test\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "X_test_single = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "# only predict 1 variable\n",
    "model_lstm = Sequential([\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(metrics=['mse'], loss='mse', optimizer='Adam')\n",
    "\n",
    "model_lstm.summary()\n",
    "\n",
    "n_epochs = 20\n",
    "history = model_lstm.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "\n",
    "model_mse=[]\n",
    "model_r2=[]\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, 6)\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "    model_lstm.fit(X_train_scale, Y_train, epochs=3, batch_size=16, verbose=0)\n",
    "    y_pred = model_lstm.predict(X_test_scale).reshape(-1, 1)\n",
    "    \n",
    "    mse=mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "lstm_mse_1=pd.DataFrame(model_mse)\n",
    "lstm_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walk-forward cv\n",
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "season = 3\n",
    "\n",
    "initial_hours = 7*24\n",
    "\n",
    "# for i in range(0, len(bee_np)+1-initial_hours-24, 24*8):\n",
    "for i in range(0, len(bee_np[season])-initial_hours-24, 24*7):\n",
    "    X_train = X[:initial_hours+i]\n",
    "    Y_train = Y[:initial_hours+i]\n",
    "    X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "    Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "    X_test_single = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "    model_lstm.fit(X_train_single, Y_train, epochs=n_epochs,\n",
    "                   batch_size=16, verbose=0)\n",
    "\n",
    "    y_pred = model_lstm.predict(X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "\n",
    "model_r2 = pd.DataFrame(model_r2)\n",
    "model_mse = pd.DataFrame(model_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single day test\n",
    "i=11*24\n",
    "\n",
    "X_train = X[:initial_hours+i]\n",
    "Y_train = Y[:initial_hours+i]\n",
    "X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_single = scaler.fit_transform(X_train)\n",
    "X_test_single = scaler.transform(X_test)\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_single, Y_train)\n",
    "\n",
    "y_pred = model_rfr.predict(X_test_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bee_np[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3294/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 3\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "# X_train = X[int(len(X)*0.8):]\n",
    "# X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "# Y_train = Y[int(len(Y)*0.8):]\n",
    "# Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "# X_train_scale = X_train\n",
    "# X_test_scale = X_test\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "X_train_single = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "X_test_single = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "# only predict 1 variable\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru.summary()\n",
    "\n",
    "n_epochs = 15\n",
    "history = model_gru.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns[weather_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# neither precip nor barometric\n",
    "weather_columns=[0, 2, 3, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse=[]\n",
    "model_r2=[]\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3, batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "    \n",
    "    mse=mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_precip_baro_r2=model_r2\n",
    "no_precip_baro_mse=model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_precip_baro_r2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_precip_baro_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# no barometric\n",
    "weather_columns = [0, 2, 3, 6, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3, batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_baro_r2=model_r2\n",
    "no_baro_mse=model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_baro_r2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_baro_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# no precip\n",
    "weather_columns = [0, 2, 3, 5, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_precip_r2 = model_r2\n",
    "no_precip_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_precip_r2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(no_precip_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# all 6\n",
    "weather_columns = [0, 2, 3, 5, 6, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "all_6_r2 = model_r2\n",
    "all_6_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(all_6_mse).mean(), pd.DataFrame(all_6_r2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# all 6\n",
    "weather_columns = [0, 2, 5, 6, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_wind_r2 = model_r2\n",
    "no_wind_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(no_wind_mse).mean(), pd.DataFrame(no_wind_r2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# just 3\n",
    "weather_columns = [0, 2, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "just_3_r2 = model_r2\n",
    "just_3_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(just_3_mse).mean(), pd.DataFrame(just_3_r2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# just 3\n",
    "weather_columns = [0, 2, 5, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_wind_precip_r2 = model_r2\n",
    "no_wind_precip_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(no_wind_precip_mse).mean(), pd.DataFrame(no_wind_precip_r2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# just 3\n",
    "weather_columns = [0, 2, 6, 8]\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True,\n",
    "        input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "kf = KFold(n_splits=tests)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "    # print(train_index)\n",
    "    # print(test_index)\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scale = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    X_test_scale = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                  batch_size=16, verbose=0)\n",
    "    y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "no_wind_baro_r2 = model_r2\n",
    "no_wind_baro_mse = model_mse\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(no_wind_baro_mse).mean(), pd.DataFrame(no_wind_baro_r2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected = [no_precip_baro_r2, no_precip_baro_mse, no_baro_r2, no_baro_mse, no_precip_r2, no_precip_mse, all_6_r2,\n",
    "                     all_6_mse, no_wind_r2, no_wind_mse, just_3_r2, just_3_mse, no_wind_precip_r2, no_wind_precip_mse, no_wind_baro_r2, no_wind_baro_mse]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_np[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "# just 3\n",
    "strong = [0, 2, 3, 8]\n",
    "# weather_columns = [0, 2, 3, 8]\n",
    "# strong = [0, 1, 2, 5]\n",
    "\n",
    "for factor in strong:\n",
    "    model_gru = Sequential([\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, 1)),\n",
    "        Dropout(0.15),\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, 1)),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_gru.compile(metrics=['mse'], loss='mse')\n",
    "    X = pd.DataFrame(weather_np[season][:, factor])\n",
    "    Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "    model_mse = []\n",
    "    model_r2 = []\n",
    "    kf = KFold(n_splits=tests)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "        # print(train_index)\n",
    "        # print(test_index)\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scale = scaler.fit_transform(X_train)\n",
    "        X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "        X_train_scale = X_train_scale.reshape(-1, 1, 1)\n",
    "        X_test_scale = X_test_scale.reshape(-1, 1, 1)\n",
    "\n",
    "        model_gru.fit(X_train_scale, Y_train, epochs=3,\n",
    "                    batch_size=16, verbose=0)\n",
    "        y_pred = model_gru.predict(X_test_scale).reshape(-1, 1)\n",
    "\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        model_mse.append(mse)\n",
    "\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        model_r2.append(r2)\n",
    "\n",
    "    features_selected.append(model_r2)\n",
    "    features_selected.append(model_mse)\n",
    "\n",
    "# gru_mse_1=pd.DataFrame(model_mse)\n",
    "# gru_r2_1=pd.DataFrame(model_r2)\n",
    "# print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features_selected)-8, len(features_selected), 2):\n",
    "    print(pd.DataFrame(features_selected[i]).mean(), pd.DataFrame(features_selected[i+1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig\n",
    "sns.boxplot(data=boxplot_df, color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio={}\n",
    "for date in arr:\n",
    "    ratio[date]=arr[date]['Active Bees']/arr[date]['Total Bees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_df=pd.DataFrame()\n",
    "line_df2=pd.DataFrame()\n",
    "for i in range(len(gru_r2_1)):\n",
    "    date = gru_r2_1[0][i]\n",
    "    line_df[date]=pd.Series(gru_r2_1[1][i])\n",
    "    line_df2[date]=pd.Series(rain_accum[date])\n",
    "    # indices.append(date)\n",
    "    # vals.append([gru_r2_1[1][i], rain_accum[date]])\n",
    "line_df=line_df.transpose()\n",
    "line_df2=line_df2.transpose()\n",
    "\n",
    "line_df.columns=['R-Squared Score']\n",
    "line_df2.columns=['Daily Accumulated Precipitation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_df=pd.DataFrame()\n",
    "line_df2=pd.DataFrame()\n",
    "for date in rain_accum:\n",
    "    if date>=pd.to_datetime('2023-01-26'):\n",
    "        line_df[date]=pd.Series(ratio[date])\n",
    "        line_df2[date]=pd.Series(rain_accum[date])\n",
    "    # indices.append(date)\n",
    "    # vals.append([gru_r2_1[1][i], rain_accum[date]])\n",
    "line_df=line_df.transpose()\n",
    "line_df2=line_df2.transpose()\n",
    "\n",
    "line_df.columns=['Daily Percent of Total Bees Active']\n",
    "line_df2.columns=['Daily Accumulated Precipitation (inches)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(line_df.iloc[:, 0].tolist(), line_df2.iloc[:, 0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# line1=sns.lineplot(data=line_df, ax=ax)\n",
    "line1 = ax.plot(line_df, label='Daily Percent of Total Bees Active')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "# df_hourly[\"Speed\"].plot(ax=ax2, c=\"r\")\n",
    "# sns.lineplot(data=line_df2, ax=ax2)\n",
    "# ax2.legend([line2], ['R-Squared Score'], loc='upper left')\n",
    "line2 = ax2.plot(line_df2, color='red', label='Daily Accumulated Precipitation (inches)')\n",
    "# line2=sns.lineplot(data=line_df2, ax=ax2, palette=['red'])\n",
    "lines = line1+line2\n",
    "labs = [l.get_label() for l in lines]\n",
    "ax.legend(lines, labs, loc='upper left')\n",
    "# fig.legend([line1, line2], ['R-Squared Score', 'Daily Accumulated Precipitation'])\n",
    "# ax.set_ylim(bottom=0, top=1)\n",
    "# ax.set_ylabel('Time')\n",
    "ax.set_ylabel('Daily Percent of Total Bees Active')\n",
    "ax2.set_ylabel('Daily Accumilated Preciptation')\n",
    "# plt.xticks([i for i in range(int((len(bee_np[0])-initial_hours)/24))])\n",
    "\n",
    "plt.title('Daily Bee Activity vs. Precipitation')\n",
    "plt.xlabel('Time')\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'r2_vs_rain.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_r2_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = pd.DataFrame(lstm_r2_1[1])\n",
    "indexed.index = lstm_r2_1[0]\n",
    "indexed.columns=['LSTM']\n",
    "indexed2 = pd.DataFrame(gru_r2_1[1])\n",
    "indexed2.index = gru_r2_1[0]\n",
    "indexed2.columns = ['GRU']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=indexed, ax=ax)\n",
    "sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "ax.set_ylim(bottom=0, top=1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('R-Squared Score')\n",
    "# plt.xticks([i for i in range(int((len(bee_np[0])-initial_hours)/24))])\n",
    "plt.title('Initial Performance (R-Squared Score)')\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'r2_initial.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = pd.DataFrame(lstm_mse_1[1])\n",
    "indexed.index = lstm_mse_1[0]\n",
    "indexed.columns=['LSTM']\n",
    "indexed2 = pd.DataFrame(gru_mse_1[1])\n",
    "indexed2.index = gru_mse_1[0]\n",
    "indexed2.columns = ['GRU']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=indexed, ax=ax)\n",
    "sns.lineplot(data=indexed2, ax=ax, palette=['red'])\n",
    "ax.set_ylim(bottom=0, top=0.05)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "# plt.xticks([i for i in range(int((len(bee_np[0])-initial_hours)/24))])\n",
    "plt.title('Initial Performance (Mean Squared Error)')\n",
    "plt.savefig(os.path.join(directory, 'Figures',\n",
    "            'mse_initial.jpg'), dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW RNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weather_np[season][:, [i for i in range(len(weather_np[0][0])-1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weather_np[season][:, weather_columns]).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weather_np[season]\n",
    "             [:, [i for i in range(len(weather_np[0][0])-1)]]).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "season = 0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, [i for i in range(len(weather_np[0][0])-1)]])\n",
    "# X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X_train = X[int(len(X)*0.8):]\n",
    "# X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "# Y_train = Y[int(len(Y)*0.8):]\n",
    "# Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "X_train = np.asarray(X).astype('float32')\n",
    "# X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y).astype('float32')\n",
    "# Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "# X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train_scale.reshape(-1, 9)\n",
    "X_test = X_test_scale.reshape(-1, 9)\n",
    "\n",
    "# model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "# model_gru.fit(X_train_single, Y_train, epochs=n_epochs,\n",
    "#                 batch_size=16, verbose=0)\n",
    "\n",
    "# y_pred = model_gru.predict(X_test_single).reshape(-1, 1)\n",
    "\n",
    "# mse = mean_squared_error(Y_test, y_pred)\n",
    "# model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "\n",
    "# r2 = r2_score(Y_test, y_pred)\n",
    "# model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "\n",
    "# model = Sequential([\n",
    "#     LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 9)),\n",
    "#     Dropout(0.15),\n",
    "#     LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 9)),\n",
    "#     Dropout(0.1),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "\n",
    "model=RandomForestRegressor()\n",
    "\n",
    "mse_scorer=make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Define the SequentialFeatureSelector object for forward selection\n",
    "sfs = SequentialFeatureSelector(estimator=model, n_features_to_select=None, scoring=mse_scorer, direction='forward', cv=None)\n",
    "\n",
    "# scores = cross_val_score(sfs, X_train, Y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the SequentialFeatureSelector object to the training data\n",
    "sfs.fit(X_train, Y_train)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_features = sfs.get_support(indices=True)\n",
    "\n",
    "# Print the selected feature indices\n",
    "print(\"Selected feature indices:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(selected_features):\n",
    "    print(weather_data.columns[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW RNN TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse=[]\n",
    "model_r2=[]\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale\n",
    "    temp_y_pred = model_rfr.predict(temp_X_test_single)\n",
    "    \n",
    "    mse=mean_squared_error(temp_y_pred, temp_Y_test)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(temp_y_pred, temp_Y_test)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "rfr_mse=pd.DataFrame(model_mse)\n",
    "rfr_r2=pd.DataFrame(model_r2)\n",
    "print(rfr_mse, rfr_mse.mean(), rfr_r2, rfr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 0\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "X_train = X[int(len(X)*0.8):]\n",
    "X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "Y_train = Y[int(len(Y)*0.8):]\n",
    "Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "\n",
    "# only predict 1 variable\n",
    "model_lstm = Sequential([\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_lstm.summary()\n",
    "\n",
    "n_epochs = 15\n",
    "history = model_lstm.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "for i in range(0, len(bee_np[season])-initial_hours-24, 24):\n",
    "    X_train = X[:initial_hours+i]\n",
    "    Y_train = Y[:initial_hours+i]\n",
    "    X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "    Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "    X_test_single = X_test_scale.reshape(-1, 1, 6)\n",
    "\n",
    "    model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    n_epochs = 15\n",
    "    model_lstm.fit(X_train_single, Y_train, epochs=n_epochs,\n",
    "                   batch_size=16, verbose=0)\n",
    "\n",
    "    y_pred = model_lstm.predict(X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(y_pred, Y_test)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(y_pred, Y_test)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "model_mse=pd.DataFrame(model_mse)\n",
    "model_r2=pd.DataFrame(model_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mse=pd.DataFrame(model_mse)\n",
    "lstm_r2=model_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2=pd.DataFrame(model_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lineplot(data = model_r2, x=model_r2.index.tolist(), y=model_r2[0].tolist())\n",
    "# g.set_yticks(range(11))\n",
    "# g.set_yticklabels([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 2\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "X_train = X[int(len(X)*0.8):]\n",
    "X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "Y_train = Y[int(len(Y)*0.8):]\n",
    "Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "\n",
    "# only predict 1 variable\n",
    "model_gru1 = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru1.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru1.summary()\n",
    "\n",
    "n_epochs = 15\n",
    "history = model_lstm.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "# for i in range(0, len(bee_np)+1-initial_hours-24, 24*8):\n",
    "for i in range(0, len(bee_np[season])-initial_hours-24, 24):\n",
    "    X_train=X[:initial_hours+i]\n",
    "    Y_train=Y[:initial_hours+i]\n",
    "    X_test=X[initial_hours+i:initial_hours+i+24]\n",
    "    Y_test=Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "    X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "\n",
    "    model_gru1.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    n_epochs = 15\n",
    "    model_gru1.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, verbose=0)\n",
    "\n",
    "    y_pred = model_gru1.predict(X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(y_pred, Y_test)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(y_pred, Y_test)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "\n",
    "    # pd.DataFrame(history.history['loss'], index=list(range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "    # pd.DataFrame(history.history['val_loss'], index=list(range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "    # print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru3_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru3_mse=pd.DataFrame(model_mse)\n",
    "gru3_r2=pd.DataFrame(model_r2)\n",
    "\n",
    "sns.lineplot(data=gru3_r2, x=gru3_r2.index.tolist(),\n",
    "             y=gru3_r2[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru2_mse=pd.DataFrame(model_mse)\n",
    "gru2_r2=pd.DataFrame(model_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=gru2_r2, x=gru2_r2.index.tolist(),\n",
    "                 y=gru2_r2[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mse=pd.DataFrame(model_mse)\n",
    "gru_r2=pd.DataFrame(model_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=gru_mse, x=gru_mse.index.tolist(),\n",
    "                 y=gru_mse[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=gru_r2, x=gru_r2.index.tolist(),\n",
    "                 y=gru_r2[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "for i in range(0, len(bee_np)+1-initial_hours-24, 24*7):\n",
    "# for i in range(0, 3350-initial_hours-24, 24*7):\n",
    "    X_train=X[:initial_hours+i]\n",
    "    Y_train=Y[:initial_hours+i]\n",
    "    X_test=X[initial_hours+i:initial_hours+i+24*7]\n",
    "    Y_test=Y[initial_hours+i:initial_hours+i+24*7]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "    X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "\n",
    "    model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    n_epochs = 15\n",
    "    model_lstm.fit(X_train_single, Y_train[['Bees Outside']], epochs=n_epochs, batch_size=16, verbose=0)\n",
    "\n",
    "    y_pred = model_lstm.predict(X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(y_pred, Y_test['Bees Outside'])\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(y_pred, Y_test['Bees Outside'])\n",
    "    model_r2.append(r2)\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "\n",
    "    # pd.DataFrame(history.history['loss'], index=list(range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "    # pd.DataFrame(history.history['val_loss'], index=list(range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "    # print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_mse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_r2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = []\n",
    "model_r2 = []\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_train_scale = scaler.fit_transform(temp_X_train)\n",
    "    temp_X_test_scale = scaler.transform(temp_X_test)\n",
    "\n",
    "    model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    n_epochs = 15\n",
    "    model_lstm.fit(X_train_single, Y_train[['Bees Outside']], epochs=n_epochs, batch_size=16, verbose=0)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale.reshape(-1, 1, 6)\n",
    "    temp_y_pred = model_lstm.predict(temp_X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_r2.append(r2)\n",
    "\n",
    "lstm1_mse = pd.DataFrame(model_mse)\n",
    "lstm1_r2 = pd.DataFrame(model_r2)\n",
    "print(lstm1_mse, lstm1_mse.mean(), lstm1_r2, lstm1_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only predict 1 variable\n",
    "model_gru1 = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru1.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru1.summary()\n",
    "\n",
    "n_epochs = 30\n",
    "history = model_gru1.fit(X_train_single, Y_train[['Bees Outside']], epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test[['Bees Outside']]), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = []\n",
    "model_r2 = []\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale.reshape(-1, 1, 6)\n",
    "    temp_y_pred = model_gru1.predict(temp_X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_r2.append(r2)\n",
    "\n",
    "gru1_mse = pd.DataFrame(model_mse)\n",
    "gru1_r2 = pd.DataFrame(model_r2)\n",
    "print(gru1_mse, gru1_mse.mean(), gru1_r2, gru1_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df = pd.concat([rfr_mse, gbr_mse, lstm1_mse, gru1_mse], axis=1)\n",
    "boxplot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df.columns=['Random Forest', 'Gradient Boosting', 'LSTM', 'GRU']\n",
    "boxplot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(palette='muted', rc={\"figure.figsize\":(8, 12)})\n",
    "sns.boxplot(data=boxplot_df, color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.ylabel('MSE')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df = pd.concat([rfr_r2, gbr_r2, lstm1_r2, gru1_r2], axis=1)\n",
    "boxplot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df.columns=['Random Forest', 'Gradient Boosting', 'LSTM', 'GRU']\n",
    "boxplot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(palette='muted', rc={\"figure.figsize\":(8, 12)})\n",
    "sns.boxplot(data=boxplot_df, color='yellow', medianprops={\"color\": \"black\"})\n",
    "plt.ylabel('R2 Score')\n",
    "# plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame([[float(rfr_mse.mean()), float(gbr_mse.mean()), float(lstm1_mse.mean()), float(gru1_mse.mean())], \n",
    "              [float(rfr_r2.mean()), float(gbr_r2.mean()), float(lstm1_r2.mean()), float(gru1_r2.mean())]],\n",
    "    index=['Average MSE', 'Average R2 Score'], \n",
    "    columns=['Random Forest', 'Gradient Boosting', 'LSTM', 'GRU']).to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                  |   Random Forest |   Gradient Boosting |     LSTM |      GRU |\n",
    "|:-----------------|----------------:|--------------------:|---------:|---------:|\n",
    "| Average MSE      |       0.0127412 |           0.0200705 | 0.025342 | 0.023604 |\n",
    "| Average R2 Score |       0.746717  |           0.56804   | 0.414296 | 0.40879  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF INITIAL TESTING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[:,1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_gbr = GradientBoostingRegressor()\n",
    "\n",
    "model_gbr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_gbr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_gbr.score(X_test_scale, Y_test))\n",
    "\n",
    "# pipe = Pipeline([('scaler', scaler), ('model', model)])\n",
    "# print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "#       cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse=[]\n",
    "model_r2=[]\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale\n",
    "    temp_y_pred = model_gbr.predict(temp_X_test_single)\n",
    "    \n",
    "    mse=mean_squared_error(temp_y_pred, temp_Y_test)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(temp_y_pred, temp_Y_test)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "gbr_mse=pd.DataFrame(model_mse)\n",
    "gbr_r2=pd.DataFrame(model_r2)\n",
    "print(gbr_mse, gbr_mse.mean(), gbr_r2, gbr_r2.mean(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[:, 1])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model_rfr = RandomForestRegressor()\n",
    "\n",
    "model_rfr.fit(X_train_scale, Y_train)\n",
    "y_pred = model_rfr.predict(X_test_scale)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print('mse: '+str(mse))\n",
    "print('R2 test data: ', model_rfr.score(X_test_scale, Y_test))\n",
    "\n",
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "print('R2 cross-validated: ', cross_val_score(pipe, X, Y, cv=10),\n",
    "      cross_val_score(pipe, X, Y, cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse=[]\n",
    "model_r2=[]\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(X,Y, test_size = 0.2, random_state = i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale\n",
    "    temp_y_pred = model_rfr.predict(temp_X_test_single)\n",
    "    \n",
    "    mse=mean_squared_error(temp_y_pred, temp_Y_test)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2=r2_score(temp_y_pred, temp_Y_test)\n",
    "    model_r2.append(r2)\n",
    "\n",
    "rfr_mse=pd.DataFrame(model_mse)\n",
    "rfr_r2=pd.DataFrame(model_r2)\n",
    "print(rfr_mse, rfr_mse.mean(), rfr_r2, rfr_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', scaler), ('model', model_rfr)])\n",
    "cvs=cross_val_score(pipe, X, Y, cv=20)\n",
    "print('R2 cross-validated: ', cvs, cvs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np, columns=['Minutes Outside', 'Bees Outside'])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "X_test_single = X_test_scale.reshape(-1,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only predict 1 variable\n",
    "model_lstm = Sequential([\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    LSTM(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_lstm.summary()\n",
    "\n",
    "n_epochs = 10\n",
    "history = model_lstm.fit(X_train_single, Y_train[['Bees Outside']], epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test[['Bees Outside']]), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = []\n",
    "model_r2 = []\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale.reshape(-1, 1, 6)\n",
    "    temp_y_pred = model_lstm.predict(temp_X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_r2.append(r2)\n",
    "\n",
    "lstm1_mse = pd.DataFrame(model_mse)\n",
    "lstm1_r2 = pd.DataFrame(model_r2)\n",
    "print(lstm1_mse, lstm1_mse.mean(), lstm1_r2, lstm1_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only predict 1 variable\n",
    "model_gru1 = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, 6)),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru1.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru1.summary()\n",
    "\n",
    "n_epochs = 30\n",
    "history = model_gru1.fit(X_train_single, Y_train[['Bees Outside']], epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test_single, Y_test[['Bees Outside']]), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = []\n",
    "model_r2 = []\n",
    "for i in range(tests):\n",
    "    temp_X_train, temp_X_test, temp_Y_train, temp_Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=i+1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    temp_X_test_scale = scaler.fit_transform(temp_X_test)\n",
    "\n",
    "    temp_X_test_single = temp_X_test_scale.reshape(-1, 1, 6)\n",
    "    temp_y_pred = model_gru1.predict(temp_X_test_single).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_mse.append(mse)\n",
    "\n",
    "    r2 = r2_score(temp_y_pred, temp_Y_test['Bees Outside'])\n",
    "    model_r2.append(r2)\n",
    "\n",
    "gru1_mse = pd.DataFrame(model_mse)\n",
    "gru1_r2 = pd.DataFrame(model_r2)\n",
    "print(gru1_mse, gru1_mse.mean(), gru1_r2, gru1_r2.mean(), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale=scaler.fit_transform(X)\n",
    "X_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "x.append([])\n",
    "x[-1].append(weather_data.iloc[4, weather_columns].values)\n",
    "x[-1].append(weather_data.iloc[5, weather_columns].values)\n",
    "x[-1].append(weather_data.iloc[6, weather_columns].values)\n",
    "x.append([])\n",
    "x[-1].append(weather_data.iloc[7, weather_columns].values)\n",
    "x[-1].append(weather_data.iloc[8, weather_columns].values)\n",
    "x[-1].append(weather_data.iloc[9, weather_columns].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(temp[0:0+time_steps]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=5\n",
    "index=0\n",
    "X2=[]\n",
    "\n",
    "temp=[]\n",
    "\n",
    "date=pd.to_datetime('2022-05-02')\n",
    "\n",
    "for i in range(index, len(weather_data)):\n",
    "    if weather_data.iloc[i].name==date-pd.Timedelta(hours=time_steps-1):\n",
    "        print('name', weather_data.iloc[i].name)\n",
    "        for j in range(i, i+time_steps+24):\n",
    "            temp.append(weather_data.iloc[j, weather_columns].values)\n",
    "        index=i\n",
    "    \n",
    "for hour in range(24):\n",
    "    X2.append(np.array(temp[hour:hour+time_steps]))\n",
    "\n",
    "# X2=np.array(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp=[]\n",
    "\n",
    "date=pd.to_datetime('2022-05-03')\n",
    "\n",
    "for i in range(index, len(weather_data)):\n",
    "    if weather_data.iloc[i].name==date-pd.Timedelta(hours=time_steps-1):\n",
    "        print('name', weather_data.iloc[i].name)\n",
    "        for j in range(i, i+time_steps+24):\n",
    "            temp.append(weather_data.iloc[j, weather_columns].values)\n",
    "        index=i\n",
    "    \n",
    "for hour in range(24):\n",
    "    X2.append(np.array(temp[hour:hour+time_steps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.iloc[48:48+24, weather_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_blocks[-1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weather_data)):\n",
    "    if weather_data.iloc[i].name==data_blocks[-1][0]-pd.Timedelta(hours=time_steps-1):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testso=np.array([1, 2, 3, 4, 5, 6]).reshape(1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(testso[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list=[]\n",
    "for i in X_test[0]:\n",
    "    temp_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list=np.array(temp_list).reshape(1, 24, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.predict(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model and validation cells\n",
    "season = 3\n",
    "\n",
    "time_steps=24\n",
    "index=0\n",
    "X2=[]\n",
    "# Y2=Y.iloc[time_steps-1:]\n",
    "\n",
    "for date in data_blocks[-1]:\n",
    "    for i in range(index, len(weather_data)):\n",
    "        if weather_data.iloc[i].name==date-pd.Timedelta(hours=time_steps-1):\n",
    "            temp=[]\n",
    "            # print('name', weather_data.iloc[i].name)\n",
    "            for j in range(i, i+time_steps+24):\n",
    "                temp.append(weather_data.iloc[j, weather_columns].values)\n",
    "            for hour in range(24):\n",
    "                X2.append(np.array(temp[hour:hour+time_steps]))\n",
    "            index=i\n",
    "            break\n",
    "X2=np.array(X2)\n",
    "\n",
    "X=X2\n",
    "# X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# X = pd.DataFrame(weather_np[:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np, columns=['Y1', \"Y2\"])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n",
    "# X_train = X[int(len(X)*0.8):]\n",
    "# X_test = X[:len(X)-int(len(X)*0.8)]\n",
    "# Y_train = Y[int(len(Y)*0.8):]\n",
    "# Y_test = Y[:len(Y)-int(len(X)*0.8)]\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "Y_train = np.asarray(Y_train).astype('float32')\n",
    "Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scale = scaler.fit_transform(X_train)\n",
    "# X_test_scale = scaler.transform(X_test)\n",
    "# X_train_scale = X_train\n",
    "# X_test_scale = X_test\n",
    "\n",
    "# X_train_single = X_train_scale.reshape(-1, 1, 6)\n",
    "# X_test_single = X_test_scale.reshape(-1,1,6)\n",
    "# X_train_single = X_train_scale.reshape(-1, time_steps, len(weather_columns))\n",
    "# X_test_single = X_test_scale.reshape(-1, time_steps, len(weather_columns))\n",
    "\n",
    "# only predict 1 variable\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', input_shape=(time_steps, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru.summary()\n",
    "\n",
    "n_epochs = 20\n",
    "history = model_gru.fit(X_train, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "    X_test, Y_test), verbose=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pd.DataFrame(history.history['loss'], index=list(\n",
    "    range(n_epochs)), columns=['Training Loss']).plot(ax=ax)\n",
    "pd.DataFrame(history.history['val_loss'], index=list(\n",
    "    range(n_epochs)), columns=['Validation Loss']).plot(ax=ax, c='r')\n",
    "\n",
    "print(history.history['val_loss'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 iterations\n",
    "#15\n",
    "model_mse = []\n",
    "model_r2 = []\n",
    "\n",
    "season=3\n",
    "\n",
    "initial_hours = 7*24\n",
    "\n",
    "# X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# for i in range(0, len(bee_np)+1-initial_hours-24, 24*8):\n",
    "for i in range(0, len(bee_np[season])-initial_hours, 24):\n",
    "    X_train = X[:initial_hours+i]\n",
    "    Y_train = Y[:initial_hours+i]\n",
    "    X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "    Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32')\n",
    "    X_test = np.asarray(X_test).astype('float32')\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    # print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    # print(type(X_train), type(X_test), type(Y_train), type(Y_test))\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train_scale = scaler.fit_transform(X_train)\n",
    "    # X_test_scale = scaler.transform(X_test)\n",
    "    \n",
    "    # X_train_single = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    # X_test_single = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train, Y_train, epochs=n_epochs,\n",
    "                   batch_size=16, verbose=0)\n",
    "\n",
    "    # y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "    y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    \n",
    "    model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "\n",
    "model_r2=pd.DataFrame(model_r2)\n",
    "model_mse=pd.DataFrame(model_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_r2_1=model_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2.to_csv(os.path.join(directory, '1Test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dates = [pd.to_datetime('2022-12-09') +\n",
    "                pd.Timedelta(days=i) for i in range(20)]\n",
    "remove_dates.append(pd.to_datetime('2022-07-20'))\n",
    "remove_dates.append(pd.to_datetime('2022-09-26'))\n",
    "remove_dates.append(pd.to_datetime('2022-11-18'))\n",
    "remove_dates.append(pd.to_datetime('2022-09-26'))\n",
    "remove_dates.append(pd.to_datetime('2023-02-06'))\n",
    "remove_dates.append(pd.to_datetime('2023-02-22'))\n",
    "remove_dates.append(pd.to_datetime('2023-02-25'))\n",
    "remove_dates.append(pd.to_datetime('2023-03-01'))\n",
    "# remove_dates.append(pd.to_datetime('2023-03-01'))\n",
    "#over .4 inches\n",
    "high_rain = [pd.to_datetime('2022-11-01 00:00:00'),\n",
    "             pd.to_datetime('2022-11-08 00:00:00'),\n",
    "             pd.to_datetime('2023-02-03 00:00:00'),\n",
    "             pd.to_datetime('2023-02-04 00:00:00'),\n",
    "             pd.to_datetime('2023-02-05 00:00:00'),\n",
    "             pd.to_datetime('2023-02-11 00:00:00'),\n",
    "             pd.to_datetime('2023-02-23 00:00:00'),\n",
    "             pd.to_datetime('2023-02-24 00:00:00'),\n",
    "             pd.to_datetime('2023-02-26 00:00:00')]\n",
    "\n",
    "for date in high_rain:\n",
    "    remove_dates.append(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2023-02-24 00:00:00') in remove_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_r2=[]\n",
    "for i in range(len(gru_r2_1)):\n",
    "    if not gru_r2_1.iloc[i, 0] in remove_dates:\n",
    "        new_r2.append(gru_r2_1.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new_r2).to_csv(os.path.join(directory, '1Test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(new_r2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_steps=48\n",
    "# X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "# Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scale=pd.DataFrame(scaler.fit_transform(X))\n",
    "# X2 = pd.concat([X_scale.shift(n) for n in range(time_steps)], axis=1).dropna()\n",
    "# X2 = X2.values.reshape(-1,time_steps, len(weather_columns))\n",
    "# Y2 = Y.iloc[time_steps-1:]\n",
    "# Y_shift=pd.concat([Y.shift(n) for n in range(time_steps)], axis=1).dropna()\n",
    "\n",
    "# # X_train, X_test, Y_train, Y_test = train_test_split(X2,Y2, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train_scale = scaler.fit_transform(X_train)\n",
    "# # X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, :, [0, 1, 2, 3, 5]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(len(weather_columns)) if i!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps=1\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns]).to_numpy().reshape(-1, time_steps, len(weather_columns))\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, :, [i for i in range(len(weather_columns)) if i!=dropped_feature]][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model_r2.append([weather_np[season][:, -1][initial_hours], 1.4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_perm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance\n",
    "feature_mse = []\n",
    "feature_r2 = []\n",
    "\n",
    "for dropped_feature in range(len(weather_columns)):\n",
    "    model_gru = Sequential([\n",
    "        GRU(16, activation='relu', return_sequences=True,\n",
    "            input_shape=(1, len(weather_columns)-1)),\n",
    "        Dropout(0.15),\n",
    "        GRU(16, activation='relu', input_shape=(1, len(weather_columns)-1)),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    X_perm = X[:, :, [i for i in range(\n",
    "        len(weather_columns)) if i != dropped_feature]]\n",
    "    model_r2 = []\n",
    "    model_mse = []\n",
    "\n",
    "    model_r2.append(X_perm[0])\n",
    "    model_mse.append(X_perm[0])\n",
    "\n",
    "    feature_mse.append(model_mse)\n",
    "    feature_r2.append(model_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2 = []\n",
    "model_mse = []\n",
    "\n",
    "X = pd.DataFrame(weather_np[season][:, weather_columns])\n",
    "Y = pd.DataFrame(bee_np[season][:, 1])\n",
    "\n",
    "model_gru = Sequential([\n",
    "    GRU(16, activation='relu', return_sequences=True, input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.15),\n",
    "    GRU(16, activation='relu', input_shape=(1, len(weather_columns))),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "model_gru.summary()\n",
    "\n",
    "for i in range(0, len(bee_np[season])-initial_hours, 24*8):\n",
    "    X_train = X[:initial_hours+i]\n",
    "    Y_train = Y[:initial_hours+i]\n",
    "    X_test = X[initial_hours+i:initial_hours+i+24]\n",
    "    Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "    X_train = np.asarray(X_train).astype('float32').reshape(-1, 1, 6)\n",
    "    X_test = np.asarray(X_test).astype('float32').reshape(-1, 1, 6)\n",
    "    Y_train = np.asarray(Y_train).astype('float32')\n",
    "    Y_test = np.asarray(Y_test).astype('float32')\n",
    "\n",
    "    # print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    # print(type(X_train), type(X_test), type(Y_train), type(Y_test))\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train_scale = scaler.fit_transform(X_train)\n",
    "    # X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "    # X_train_single = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "    # X_test_single = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "    model_gru.fit(X_train, Y_train, epochs=n_epochs,\n",
    "                    batch_size=16, verbose=0)\n",
    "\n",
    "    # y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "    y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "\n",
    "    model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "\n",
    "feature_mse.append(model_mse)\n",
    "feature_r2.append(model_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance\n",
    "feature_mse=[]\n",
    "feature_r2=[]\n",
    "\n",
    "for dropped_feature in range(len(weather_columns)):\n",
    "    model_gru = Sequential([\n",
    "        GRU(16, activation='relu', return_sequences=True, input_shape=(1, len(weather_columns)-1)),\n",
    "        Dropout(0.15),\n",
    "        GRU(16, activation='relu', input_shape=(1, len(weather_columns)-1)),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model_gru.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "    X_perm = X[:, :, [i for i in range(len(weather_columns)) if i!=dropped_feature]]\n",
    "    model_r2=[]\n",
    "    model_mse=[]\n",
    "\n",
    "# adjust\n",
    "    for i in range(0, len(bee_np[season])-initial_hours, 24*8):\n",
    "        X_train = X_perm[:initial_hours+i]\n",
    "        Y_train = Y[:initial_hours+i]\n",
    "        X_test = X_perm[initial_hours+i:initial_hours+i+24]\n",
    "        Y_test = Y[initial_hours+i:initial_hours+i+24]\n",
    "\n",
    "        X_train = np.asarray(X_train).astype('float32')\n",
    "        X_test = np.asarray(X_test).astype('float32')\n",
    "        Y_train = np.asarray(Y_train).astype('float32')\n",
    "        Y_test = np.asarray(Y_test).astype('float32')\n",
    "        \n",
    "\n",
    "        # print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "        # print(type(X_train), type(X_test), type(Y_train), type(Y_test))\n",
    "\n",
    "        # scaler = StandardScaler()\n",
    "        # X_train_scale = scaler.fit_transform(X_train)\n",
    "        # X_test_scale = scaler.transform(X_test)\n",
    "        \n",
    "        # X_train_single = X_train_scale.reshape(-1, 1, len(weather_columns))\n",
    "        # X_test_single = X_test_scale.reshape(-1, 1, len(weather_columns))\n",
    "\n",
    "        model_gru.fit(X_train, Y_train, epochs=n_epochs,\n",
    "                    batch_size=16, verbose=0)\n",
    "\n",
    "        # y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "        y_pred = model_gru.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "        mse = mean_squared_error(Y_test, y_pred)\n",
    "        \n",
    "        model_mse.append([weather_np[season][:, -1][initial_hours+i], mse])\n",
    "\n",
    "        r2 = r2_score(Y_test, y_pred)\n",
    "        model_r2.append([bee_np[season][:, -1][initial_hours+i], r2])\n",
    "\n",
    "    feature_mse.append(model_mse)\n",
    "    feature_r2.append(model_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_r2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_r2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_r2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_r2[3]).iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns[weather_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_r2)):\n",
    "    print(pd.DataFrame(feature_r2[i]).drop(index=[14]).iloc[:, 1].mean())\n",
    "    # print(pd.DataFrame(i[:, 1]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_r2)):\n",
    "    print(pd.DataFrame(feature_r2[i]).drop(index=[14]).iloc[:, 1].mean())\n",
    "    # print(pd.DataFrame(i[:, 1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    n_epochs = 15\n",
    "    history = model_gru.fit(X_train_single, Y_train, epochs=n_epochs, batch_size=16, validation_data=(\n",
    "        X_test_single, Y_test), verbose=1)\n",
    "\n",
    "\n",
    "    # Calculate feature importances\n",
    "    feature_importances = np.zeros(X_val.shape[2])\n",
    "    for i in range(X_val.shape[2]):\n",
    "        X_val_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_val_permuted[:, :, i])\n",
    "        mse_permuted = mean_squared_error(Y_val, model.predict(X_val_permuted))\n",
    "        feature_importances[i] = baseline_mse - mse_permuted\n",
    "\n",
    "    # Rank features by importance and select subset\n",
    "    ranked_features = np.argsort(feature_importances)[::-1]\n",
    "    selected_features = []\n",
    "    for i in ranked_features:\n",
    "        selected_features.append(i)\n",
    "        X_train_subset = X_train[:, :, selected_features]\n",
    "        X_val_subset = X_val[:, :, selected_features]\n",
    "        model.fit(X_train_subset, Y_train, epochs=25, batch_size=16, verbose=1)\n",
    "        mse_subset = mean_squared_error(Y_val, model.predict(X_val_subset))\n",
    "        if mse_subset > baseline_mse:\n",
    "            break\n",
    "        baseline_mse = mse_subset\n",
    "\n",
    "    print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = Add()([model_lstm.output, model_gru.output])\n",
    "combined = Dense(1)(combined)\n",
    "\n",
    "combined_model = Model([model_lstm.input, model_gru.input], combined)\n",
    "\n",
    "combined_model.compile(metrics=['mse'], loss='mse')\n",
    "\n",
    "combined_model.summary()\n",
    "\n",
    "n_epochs = 20\n",
    "history = combined_model.fit([X_train, X_train], Y_train[['Minutes Outside']], epochs=n_epochs, batch_size=16, validation_data = ([X_test, X_test], Y_test[['Minutes Outside']]), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "21581d6d3ae79c11599e3d24eb0208c5d094f55ad0f08513ff675c72b86db4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
